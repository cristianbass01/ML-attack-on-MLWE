{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from ml_attack import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training debug:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural solver model for Module-LWE using both Fourier mapping and FFT transformation.\n",
    "class LinearComplex(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        \"\"\"\n",
    "        n: Secret dimension (e.g., 8)\n",
    "        q: Modulus\n",
    "        \"\"\"\n",
    "        super(LinearComplex, self).__init__()\n",
    "        self.q = params['q']\n",
    "        self.n = params['n']\n",
    "        self.k = params['k']\n",
    "        self.secret_type = params['secret_type']\n",
    "        mean_s, _, std_s = get_vector_distribution(params, self.secret_type, params.get('hw', -1))\n",
    "\n",
    "        self.guessed_secret = nn.Parameter(nn.init.normal_(torch.empty(self.n * self.k, dtype=torch.float), mean=mean_s, std=std_s), requires_grad=True)\n",
    "\n",
    "        self.C = nn.Parameter(nn.init.normal_(torch.empty(self.n * self.k, dtype=torch.float), mean=0, std=1), requires_grad=True)\n",
    "\n",
    "    def forward(self, A_batch):\n",
    "        \n",
    "        return torch.tensordot(A_batch, self.guessed_secret, dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, params, n_epochs=10, lr=0.01, check_every=10):\n",
    "    # Get secret dimension from first sample.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # Get the secret from the dataset\n",
    "    secret_np = dataset.get_secret()\n",
    "    secret = torch.tensor(secret_np, dtype=torch.float, device=device).view(-1)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    A = torch.tensor(dataset.get_A(), dtype=torch.float, device=device)\n",
    "    b = torch.tensor(dataset.get_B(), dtype=torch.float, device=device)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        optimizer.zero_grad()\n",
    "        pred_b = model(A)\n",
    "\n",
    "        b_loss = loss_fn(pred_b, b)\n",
    "            \n",
    "        b_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}/{n_epochs}, Loss: {b_loss:.4f}\")\n",
    "\n",
    "        if epoch % check_every == 0:\n",
    "            with torch.no_grad():\n",
    "                guessed_secret = model.guessed_secret.round().cpu().numpy()\n",
    "                if check_secret(guessed_secret, dataset.get_A(), dataset.get_B(), params):\n",
    "                    print(f\"Secret guessed correctly at epoch {epoch}!\")\n",
    "                    break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n': 256,\n",
    "    'q': 3329,\n",
    "    'k': 4,\n",
    "    'secret_type': 'cbd',\n",
    "    'error_type': 'cbd',\n",
    "    'eta': 2,\n",
    "    'mod_q': False,\n",
    "}\n",
    "\n",
    "lwe_dataset = LWEDataset(params)\n",
    "lwe_dataset.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20000, Loss: 6824699904.0000\n",
      "Epoch 2/20000, Loss: 851795574784.0000\n",
      "Epoch 3/20000, Loss: 58348142592.0000\n",
      "Epoch 4/20000, Loss: 190285938688.0000\n",
      "Epoch 5/20000, Loss: 485796085760.0000\n",
      "Epoch 6/20000, Loss: 342155427840.0000\n",
      "Epoch 7/20000, Loss: 80125345792.0000\n",
      "Epoch 8/20000, Loss: 8454282752.0000\n",
      "Epoch 9/20000, Loss: 137255878656.0000\n",
      "Epoch 10/20000, Loss: 246357442560.0000\n",
      "Epoch 11/20000, Loss: 203243945984.0000\n",
      "Epoch 12/20000, Loss: 78626504704.0000\n",
      "Epoch 13/20000, Loss: 4036806656.0000\n",
      "Epoch 14/20000, Loss: 34003214336.0000\n",
      "Epoch 15/20000, Loss: 108655624192.0000\n",
      "Epoch 16/20000, Loss: 134597369856.0000\n",
      "Epoch 17/20000, Loss: 88845467648.0000\n",
      "Epoch 18/20000, Loss: 24463495168.0000\n",
      "Epoch 19/20000, Loss: 2375588608.0000\n",
      "Epoch 20/20000, Loss: 31982256128.0000\n",
      "Epoch 21/20000, Loss: 69662031872.0000\n",
      "Epoch 22/20000, Loss: 71592394752.0000\n",
      "Epoch 23/20000, Loss: 38659407872.0000\n",
      "Epoch 24/20000, Loss: 6889768960.0000\n",
      "Epoch 25/20000, Loss: 5246318592.0000\n",
      "Epoch 26/20000, Loss: 27482828800.0000\n",
      "Epoch 27/20000, Loss: 43715133440.0000\n",
      "Epoch 28/20000, Loss: 35597348864.0000\n",
      "Epoch 29/20000, Loss: 13777272832.0000\n",
      "Epoch 30/20000, Loss: 1752633344.0000\n",
      "Epoch 31/20000, Loss: 8952715264.0000\n",
      "Epoch 32/20000, Loss: 22664024064.0000\n",
      "Epoch 33/20000, Loss: 25223663616.0000\n",
      "Epoch 34/20000, Loss: 14342080512.0000\n",
      "Epoch 35/20000, Loss: 3155039232.0000\n",
      "Epoch 36/20000, Loss: 3119067136.0000\n",
      "Epoch 37/20000, Loss: 11333175296.0000\n",
      "Epoch 38/20000, Loss: 15993879552.0000\n",
      "Epoch 39/20000, Loss: 11424303104.0000\n",
      "Epoch 40/20000, Loss: 3724591872.0000\n",
      "Epoch 41/20000, Loss: 1748823040.0000\n",
      "Epoch 42/20000, Loss: 6189424640.0000\n",
      "Epoch 43/20000, Loss: 9984644096.0000\n",
      "Epoch 44/20000, Loss: 8064983040.0000\n",
      "Epoch 45/20000, Loss: 3229635072.0000\n",
      "Epoch 46/20000, Loss: 1454821504.0000\n",
      "Epoch 47/20000, Loss: 3980087296.0000\n",
      "Epoch 48/20000, Loss: 6486396928.0000\n",
      "Epoch 49/20000, Loss: 5428183552.0000\n",
      "Epoch 50/20000, Loss: 2431017984.0000\n",
      "Epoch 51/20000, Loss: 1375807616.0000\n",
      "Epoch 52/20000, Loss: 3009924608.0000\n",
      "Epoch 53/20000, Loss: 4462810624.0000\n",
      "Epoch 54/20000, Loss: 3604059648.0000\n",
      "Epoch 55/20000, Loss: 1754192256.0000\n",
      "Epoch 56/20000, Loss: 1359740160.0000\n",
      "Epoch 57/20000, Loss: 2510584832.0000\n",
      "Epoch 58/20000, Loss: 3194638336.0000\n",
      "Epoch 59/20000, Loss: 2393776640.0000\n",
      "Epoch 60/20000, Loss: 1326925312.0000\n",
      "Epoch 61/20000, Loss: 1390042880.0000\n",
      "Epoch 62/20000, Loss: 2171087104.0000\n",
      "Epoch 63/20000, Loss: 2324583424.0000\n",
      "Epoch 64/20000, Loss: 1635342080.0000\n",
      "Epoch 65/20000, Loss: 1139669248.0000\n",
      "Epoch 66/20000, Loss: 1426264832.0000\n",
      "Epoch 67/20000, Loss: 1855405952.0000\n",
      "Epoch 68/20000, Loss: 1695056640.0000\n",
      "Epoch 69/20000, Loss: 1211417344.0000\n",
      "Epoch 70/20000, Loss: 1109071104.0000\n",
      "Epoch 71/20000, Loss: 1408991616.0000\n",
      "Epoch 72/20000, Loss: 1530354304.0000\n",
      "Epoch 73/20000, Loss: 1265982464.0000\n",
      "Epoch 74/20000, Loss: 1033575936.0000\n",
      "Epoch 75/20000, Loss: 1133434112.0000\n",
      "Epoch 76/20000, Loss: 1306903040.0000\n",
      "Epoch 77/20000, Loss: 1229327360.0000\n",
      "Epoch 78/20000, Loss: 1025059456.0000\n",
      "Epoch 79/20000, Loss: 997259520.0000\n",
      "Epoch 80/20000, Loss: 1121377664.0000\n",
      "Epoch 81/20000, Loss: 1139177600.0000\n",
      "Epoch 82/20000, Loss: 1008295808.0000\n",
      "Epoch 83/20000, Loss: 933059584.0000\n",
      "Epoch 84/20000, Loss: 994352704.0000\n",
      "Epoch 85/20000, Loss: 1039952640.0000\n",
      "Epoch 86/20000, Loss: 971107776.0000\n",
      "Epoch 87/20000, Loss: 894329728.0000\n",
      "Epoch 88/20000, Loss: 911583744.0000\n",
      "Epoch 89/20000, Loss: 952981952.0000\n",
      "Epoch 90/20000, Loss: 921898112.0000\n",
      "Epoch 91/20000, Loss: 860269952.0000\n",
      "Epoch 92/20000, Loss: 854582912.0000\n",
      "Epoch 93/20000, Loss: 882830976.0000\n",
      "Epoch 94/20000, Loss: 870306240.0000\n",
      "Epoch 95/20000, Loss: 825359168.0000\n",
      "Epoch 96/20000, Loss: 810760320.0000\n",
      "Epoch 97/20000, Loss: 826768384.0000\n",
      "Epoch 98/20000, Loss: 821466752.0000\n",
      "Epoch 99/20000, Loss: 789464000.0000\n",
      "Epoch 100/20000, Loss: 773141504.0000\n",
      "Epoch 101/20000, Loss: 780454784.0000\n",
      "Epoch 102/20000, Loss: 776968320.0000\n",
      "Epoch 103/20000, Loss: 753760128.0000\n",
      "Epoch 104/20000, Loss: 738707584.0000\n",
      "Epoch 105/20000, Loss: 740608896.0000\n",
      "Epoch 106/20000, Loss: 736824384.0000\n",
      "Epoch 107/20000, Loss: 719234112.0000\n",
      "Epoch 108/20000, Loss: 706258944.0000\n",
      "Epoch 109/20000, Loss: 704988032.0000\n",
      "Epoch 110/20000, Loss: 700320064.0000\n",
      "Epoch 111/20000, Loss: 686289024.0000\n",
      "Epoch 112/20000, Loss: 675335808.0000\n",
      "Epoch 113/20000, Loss: 672276544.0000\n",
      "Epoch 114/20000, Loss: 666777344.0000\n",
      "Epoch 115/20000, Loss: 655085376.0000\n",
      "Epoch 116/20000, Loss: 645814272.0000\n",
      "Epoch 117/20000, Loss: 641735232.0000\n",
      "Epoch 118/20000, Loss: 635626496.0000\n",
      "Epoch 119/20000, Loss: 625563008.0000\n",
      "Epoch 120/20000, Loss: 617595648.0000\n",
      "Epoch 121/20000, Loss: 612915136.0000\n",
      "Epoch 122/20000, Loss: 606451712.0000\n",
      "Epoch 123/20000, Loss: 597616128.0000\n",
      "Epoch 124/20000, Loss: 590625792.0000\n",
      "Epoch 125/20000, Loss: 585573696.0000\n",
      "Epoch 126/20000, Loss: 578977152.0000\n",
      "Epoch 127/20000, Loss: 571122176.0000\n",
      "Epoch 128/20000, Loss: 564837248.0000\n",
      "Epoch 129/20000, Loss: 559548416.0000\n",
      "Epoch 130/20000, Loss: 552999168.0000\n",
      "Epoch 131/20000, Loss: 545955968.0000\n",
      "Epoch 132/20000, Loss: 540165056.0000\n",
      "Epoch 133/20000, Loss: 534740544.0000\n",
      "Epoch 134/20000, Loss: 528379232.0000\n",
      "Epoch 135/20000, Loss: 522012096.0000\n",
      "Epoch 136/20000, Loss: 516554368.0000\n",
      "Epoch 137/20000, Loss: 511082688.0000\n",
      "Epoch 138/20000, Loss: 505006656.0000\n",
      "Epoch 139/20000, Loss: 499190848.0000\n",
      "Epoch 140/20000, Loss: 493954208.0000\n",
      "Epoch 141/20000, Loss: 488522400.0000\n",
      "Epoch 142/20000, Loss: 482788448.0000\n",
      "Epoch 143/20000, Loss: 477408512.0000\n",
      "Epoch 144/20000, Loss: 472327680.0000\n",
      "Epoch 145/20000, Loss: 467017824.0000\n",
      "Epoch 146/20000, Loss: 461642080.0000\n",
      "Epoch 147/20000, Loss: 456594112.0000\n",
      "Epoch 148/20000, Loss: 451644416.0000\n",
      "Epoch 149/20000, Loss: 446524800.0000\n",
      "Epoch 150/20000, Loss: 441489344.0000\n",
      "Epoch 151/20000, Loss: 436691328.0000\n",
      "Epoch 152/20000, Loss: 431880448.0000\n",
      "Epoch 153/20000, Loss: 426996960.0000\n",
      "Epoch 154/20000, Loss: 422260992.0000\n",
      "Epoch 155/20000, Loss: 417658368.0000\n",
      "Epoch 156/20000, Loss: 413011296.0000\n",
      "Epoch 157/20000, Loss: 408382016.0000\n",
      "Epoch 158/20000, Loss: 403896416.0000\n",
      "Epoch 159/20000, Loss: 399463872.0000\n",
      "Epoch 160/20000, Loss: 395008000.0000\n",
      "Epoch 161/20000, Loss: 390625856.0000\n",
      "Epoch 162/20000, Loss: 386347552.0000\n",
      "Epoch 163/20000, Loss: 382082496.0000\n",
      "Epoch 164/20000, Loss: 377834848.0000\n",
      "Epoch 165/20000, Loss: 373676192.0000\n",
      "Epoch 166/20000, Loss: 369577312.0000\n",
      "Epoch 167/20000, Loss: 365488576.0000\n",
      "Epoch 168/20000, Loss: 361450272.0000\n",
      "Epoch 169/20000, Loss: 357487424.0000\n",
      "Epoch 170/20000, Loss: 353556640.0000\n",
      "Epoch 171/20000, Loss: 349652448.0000\n",
      "Epoch 172/20000, Loss: 345811200.0000\n",
      "Epoch 173/20000, Loss: 342022400.0000\n",
      "Epoch 174/20000, Loss: 338259168.0000\n",
      "Epoch 175/20000, Loss: 334539584.0000\n",
      "Epoch 176/20000, Loss: 330877312.0000\n",
      "Epoch 177/20000, Loss: 327250912.0000\n",
      "Epoch 178/20000, Loss: 323656928.0000\n",
      "Epoch 179/20000, Loss: 320113216.0000\n",
      "Epoch 180/20000, Loss: 316614080.0000\n",
      "Epoch 181/20000, Loss: 313146208.0000\n",
      "Epoch 182/20000, Loss: 309718944.0000\n",
      "Epoch 183/20000, Loss: 306338048.0000\n",
      "Epoch 184/20000, Loss: 302992352.0000\n",
      "Epoch 185/20000, Loss: 299681024.0000\n",
      "Epoch 186/20000, Loss: 296412640.0000\n",
      "Epoch 187/20000, Loss: 293182816.0000\n",
      "Epoch 188/20000, Loss: 289985696.0000\n",
      "Epoch 189/20000, Loss: 286826528.0000\n",
      "Epoch 190/20000, Loss: 283706656.0000\n",
      "Epoch 191/20000, Loss: 280620160.0000\n",
      "Epoch 192/20000, Loss: 277567808.0000\n",
      "Epoch 193/20000, Loss: 274553184.0000\n",
      "Epoch 194/20000, Loss: 271572928.0000\n",
      "Epoch 195/20000, Loss: 268624896.0000\n",
      "Epoch 196/20000, Loss: 265712096.0000\n",
      "Epoch 197/20000, Loss: 262834144.0000\n",
      "Epoch 198/20000, Loss: 259988576.0000\n",
      "Epoch 199/20000, Loss: 257177696.0000\n",
      "Epoch 200/20000, Loss: 254405536.0000\n",
      "Epoch 201/20000, Loss: 251677312.0000\n",
      "Epoch 202/20000, Loss: 249011824.0000\n",
      "Epoch 203/20000, Loss: 246460352.0000\n",
      "Epoch 204/20000, Loss: 244154048.0000\n",
      "Epoch 205/20000, Loss: 242446512.0000\n",
      "Epoch 206/20000, Loss: 242294144.0000\n",
      "Epoch 207/20000, Loss: 246222944.0000\n",
      "Epoch 208/20000, Loss: 260393072.0000\n",
      "Epoch 209/20000, Loss: 296460096.0000\n",
      "Epoch 210/20000, Loss: 362099584.0000\n",
      "Epoch 211/20000, Loss: 423454080.0000\n",
      "Epoch 212/20000, Loss: 398040416.0000\n",
      "Epoch 213/20000, Loss: 284179840.0000\n",
      "Epoch 214/20000, Loss: 218627136.0000\n",
      "Epoch 215/20000, Loss: 267263840.0000\n",
      "Epoch 216/20000, Loss: 325951680.0000\n",
      "Epoch 217/20000, Loss: 291710272.0000\n",
      "Epoch 218/20000, Loss: 219807520.0000\n",
      "Epoch 219/20000, Loss: 219463040.0000\n",
      "Epoch 220/20000, Loss: 266972896.0000\n",
      "Epoch 221/20000, Loss: 266075504.0000\n",
      "Epoch 222/20000, Loss: 216976560.0000\n",
      "Epoch 223/20000, Loss: 200836576.0000\n",
      "Epoch 224/20000, Loss: 229906288.0000\n",
      "Epoch 225/20000, Loss: 239576464.0000\n",
      "Epoch 226/20000, Loss: 209731488.0000\n",
      "Epoch 227/20000, Loss: 190239616.0000\n",
      "Epoch 228/20000, Loss: 204907296.0000\n",
      "Epoch 229/20000, Loss: 216954224.0000\n",
      "Epoch 230/20000, Loss: 201160864.0000\n",
      "Epoch 231/20000, Loss: 182890592.0000\n",
      "Epoch 232/20000, Loss: 187013312.0000\n",
      "Epoch 233/20000, Loss: 197720928.0000\n",
      "Epoch 234/20000, Loss: 191948448.0000\n",
      "Epoch 235/20000, Loss: 177372992.0000\n",
      "Epoch 236/20000, Loss: 174232256.0000\n",
      "Epoch 237/20000, Loss: 181183168.0000\n",
      "Epoch 238/20000, Loss: 181773120.0000\n",
      "Epoch 239/20000, Loss: 172566048.0000\n",
      "Epoch 240/20000, Loss: 165672992.0000\n",
      "Epoch 241/20000, Loss: 167494816.0000\n",
      "Epoch 242/20000, Loss: 170474128.0000\n",
      "Epoch 243/20000, Loss: 166968448.0000\n",
      "Epoch 244/20000, Loss: 160200864.0000\n",
      "Epoch 245/20000, Loss: 157438256.0000\n",
      "Epoch 246/20000, Loss: 159000032.0000\n",
      "Epoch 247/20000, Loss: 159291568.0000\n",
      "Epoch 248/20000, Loss: 155613088.0000\n",
      "Epoch 249/20000, Loss: 151132000.0000\n",
      "Epoch 250/20000, Loss: 149479424.0000\n",
      "Epoch 251/20000, Loss: 149966944.0000\n",
      "Epoch 252/20000, Loss: 149392656.0000\n",
      "Epoch 253/20000, Loss: 146563840.0000\n",
      "Epoch 254/20000, Loss: 143314752.0000\n",
      "Epoch 255/20000, Loss: 141643552.0000\n",
      "Epoch 256/20000, Loss: 141321664.0000\n",
      "Epoch 257/20000, Loss: 140650144.0000\n",
      "Epoch 258/20000, Loss: 138704784.0000\n",
      "Epoch 259/20000, Loss: 136185248.0000\n",
      "Epoch 260/20000, Loss: 134291808.0000\n",
      "Epoch 261/20000, Loss: 133337232.0000\n",
      "Epoch 262/20000, Loss: 132623208.0000\n",
      "Epoch 263/20000, Loss: 131378296.0000\n",
      "Epoch 264/20000, Loss: 129536840.0000\n",
      "Epoch 265/20000, Loss: 127639104.0000\n",
      "Epoch 266/20000, Loss: 126181016.0000\n",
      "Epoch 267/20000, Loss: 125168672.0000\n",
      "Epoch 268/20000, Loss: 124227968.0000\n",
      "Epoch 269/20000, Loss: 123019552.0000\n",
      "Epoch 270/20000, Loss: 121516576.0000\n",
      "Epoch 271/20000, Loss: 119951144.0000\n",
      "Epoch 272/20000, Loss: 118566464.0000\n",
      "Epoch 273/20000, Loss: 117426072.0000\n",
      "Epoch 274/20000, Loss: 116410656.0000\n",
      "Epoch 275/20000, Loss: 115349568.0000\n",
      "Epoch 276/20000, Loss: 114152496.0000\n",
      "Epoch 277/20000, Loss: 112849488.0000\n",
      "Epoch 278/20000, Loss: 111540328.0000\n",
      "Epoch 279/20000, Loss: 110314016.0000\n",
      "Epoch 280/20000, Loss: 109198128.0000\n",
      "Epoch 281/20000, Loss: 108159328.0000\n",
      "Epoch 282/20000, Loss: 107138072.0000\n",
      "Epoch 283/20000, Loss: 106086992.0000\n",
      "Epoch 284/20000, Loss: 104991032.0000\n",
      "Epoch 285/20000, Loss: 103865320.0000\n",
      "Epoch 286/20000, Loss: 102739464.0000\n",
      "Epoch 287/20000, Loss: 101640504.0000\n",
      "Epoch 288/20000, Loss: 100582616.0000\n",
      "Epoch 289/20000, Loss: 99565672.0000\n",
      "Epoch 290/20000, Loss: 98579728.0000\n",
      "Epoch 291/20000, Loss: 97611616.0000\n",
      "Epoch 292/20000, Loss: 96650128.0000\n",
      "Epoch 293/20000, Loss: 95688680.0000\n",
      "Epoch 294/20000, Loss: 94725408.0000\n",
      "Epoch 295/20000, Loss: 93761936.0000\n",
      "Epoch 296/20000, Loss: 92801568.0000\n",
      "Epoch 297/20000, Loss: 91847840.0000\n",
      "Epoch 298/20000, Loss: 90903624.0000\n",
      "Epoch 299/20000, Loss: 89970816.0000\n",
      "Epoch 300/20000, Loss: 89050272.0000\n",
      "Epoch 301/20000, Loss: 88142208.0000\n",
      "Epoch 302/20000, Loss: 87246312.0000\n",
      "Epoch 303/20000, Loss: 86362128.0000\n",
      "Epoch 304/20000, Loss: 85489136.0000\n",
      "Epoch 305/20000, Loss: 84626912.0000\n",
      "Epoch 306/20000, Loss: 83775128.0000\n",
      "Epoch 307/20000, Loss: 82933680.0000\n",
      "Epoch 308/20000, Loss: 82102664.0000\n",
      "Epoch 309/20000, Loss: 81282512.0000\n",
      "Epoch 310/20000, Loss: 80474152.0000\n",
      "Epoch 311/20000, Loss: 79679344.0000\n",
      "Epoch 312/20000, Loss: 78901376.0000\n",
      "Epoch 313/20000, Loss: 78146224.0000\n",
      "Epoch 314/20000, Loss: 77424976.0000\n",
      "Epoch 315/20000, Loss: 76758320.0000\n",
      "Epoch 316/20000, Loss: 76185568.0000\n",
      "Epoch 317/20000, Loss: 75782472.0000\n",
      "Epoch 318/20000, Loss: 75697328.0000\n",
      "Epoch 319/20000, Loss: 76224424.0000\n",
      "Epoch 320/20000, Loss: 77954912.0000\n",
      "Epoch 321/20000, Loss: 82085672.0000\n",
      "Epoch 322/20000, Loss: 91039768.0000\n",
      "Epoch 323/20000, Loss: 109645840.0000\n",
      "Epoch 324/20000, Loss: 147066368.0000\n",
      "Epoch 325/20000, Loss: 218646400.0000\n",
      "Epoch 326/20000, Loss: 342524928.0000\n",
      "Epoch 327/20000, Loss: 516930432.0000\n",
      "Epoch 328/20000, Loss: 670899968.0000\n",
      "Epoch 329/20000, Loss: 657839872.0000\n",
      "Epoch 330/20000, Loss: 416062912.0000\n",
      "Epoch 331/20000, Loss: 136999824.0000\n",
      "Epoch 332/20000, Loss: 75374584.0000\n",
      "Epoch 333/20000, Loss: 230468208.0000\n",
      "Epoch 334/20000, Loss: 372771040.0000\n",
      "Epoch 335/20000, Loss: 320171904.0000\n",
      "Epoch 336/20000, Loss: 143752304.0000\n",
      "Epoch 337/20000, Loss: 61707552.0000\n",
      "Epoch 338/20000, Loss: 141507616.0000\n",
      "Epoch 339/20000, Loss: 238512672.0000\n",
      "Epoch 340/20000, Loss: 213180464.0000\n",
      "Epoch 341/20000, Loss: 106281416.0000\n",
      "Epoch 342/20000, Loss: 58945200.0000\n",
      "Epoch 343/20000, Loss: 110734848.0000\n",
      "Epoch 344/20000, Loss: 167025728.0000\n",
      "Epoch 345/20000, Loss: 145827168.0000\n",
      "Epoch 346/20000, Loss: 80654960.0000\n",
      "Epoch 347/20000, Loss: 56564576.0000\n",
      "Epoch 348/20000, Loss: 90290240.0000\n",
      "Epoch 349/20000, Loss: 122249624.0000\n",
      "Epoch 350/20000, Loss: 107005552.0000\n",
      "Epoch 351/20000, Loss: 67640776.0000\n",
      "Epoch 352/20000, Loss: 53564776.0000\n",
      "Epoch 353/20000, Loss: 73479664.0000\n",
      "Epoch 354/20000, Loss: 92915152.0000\n",
      "Epoch 355/20000, Loss: 85022872.0000\n",
      "Epoch 356/20000, Loss: 61402448.0000\n",
      "Epoch 357/20000, Loss: 50463608.0000\n",
      "Epoch 358/20000, Loss: 60198716.0000\n",
      "Epoch 359/20000, Loss: 73058904.0000\n",
      "Epoch 360/20000, Loss: 71561520.0000\n",
      "Epoch 361/20000, Loss: 58184712.0000\n",
      "Epoch 362/20000, Loss: 48427976.0000\n",
      "Epoch 363/20000, Loss: 50784320.0000\n",
      "Epoch 364/20000, Loss: 58882160.0000\n",
      "Epoch 365/20000, Loss: 61552616.0000\n",
      "Epoch 366/20000, Loss: 55727232.0000\n",
      "Epoch 367/20000, Loss: 47925844.0000\n",
      "Epoch 368/20000, Loss: 45454052.0000\n",
      "Epoch 369/20000, Loss: 48782384.0000\n",
      "Epoch 370/20000, Loss: 52563768.0000\n",
      "Epoch 371/20000, Loss: 52156020.0000\n",
      "Epoch 372/20000, Loss: 47942960.0000\n",
      "Epoch 373/20000, Loss: 43867512.0000\n",
      "Epoch 374/20000, Loss: 42966240.0000\n",
      "Epoch 375/20000, Loss: 44787240.0000\n",
      "Epoch 376/20000, Loss: 46574536.0000\n",
      "Epoch 377/20000, Loss: 46203872.0000\n",
      "Epoch 378/20000, Loss: 43865972.0000\n",
      "Epoch 379/20000, Loss: 41382368.0000\n",
      "Epoch 380/20000, Loss: 40329396.0000\n",
      "Epoch 381/20000, Loss: 40818728.0000\n",
      "Epoch 382/20000, Loss: 41745608.0000\n",
      "Epoch 383/20000, Loss: 41920744.0000\n",
      "Epoch 384/20000, Loss: 40982936.0000\n",
      "Epoch 385/20000, Loss: 39470028.0000\n",
      "Epoch 386/20000, Loss: 38222200.0000\n",
      "Epoch 387/20000, Loss: 37723464.0000\n",
      "Epoch 388/20000, Loss: 37857260.0000\n",
      "Epoch 389/20000, Loss: 38127376.0000\n",
      "Epoch 390/20000, Loss: 38067588.0000\n",
      "Epoch 391/20000, Loss: 37524008.0000\n",
      "Epoch 392/20000, Loss: 36670984.0000\n",
      "Epoch 393/20000, Loss: 35827600.0000\n",
      "Epoch 394/20000, Loss: 35240260.0000\n",
      "Epoch 395/20000, Loss: 34965648.0000\n",
      "Epoch 396/20000, Loss: 34888852.0000\n",
      "Epoch 397/20000, Loss: 34826092.0000\n",
      "Epoch 398/20000, Loss: 34632880.0000\n",
      "Epoch 399/20000, Loss: 34261852.0000\n",
      "Epoch 400/20000, Loss: 33758292.0000\n",
      "Epoch 401/20000, Loss: 33215260.0000\n",
      "Epoch 402/20000, Loss: 32721496.0000\n",
      "Epoch 403/20000, Loss: 32327024.0000\n",
      "Epoch 404/20000, Loss: 32034898.0000\n",
      "Epoch 405/20000, Loss: 31813540.0000\n",
      "Epoch 406/20000, Loss: 31617768.0000\n",
      "Epoch 407/20000, Loss: 31407690.0000\n",
      "Epoch 408/20000, Loss: 31159466.0000\n",
      "Epoch 409/20000, Loss: 30867324.0000\n",
      "Epoch 410/20000, Loss: 30539356.0000\n",
      "Epoch 411/20000, Loss: 30190824.0000\n",
      "Epoch 412/20000, Loss: 29837906.0000\n",
      "Epoch 413/20000, Loss: 29493564.0000\n",
      "Epoch 414/20000, Loss: 29165760.0000\n",
      "Epoch 415/20000, Loss: 28857598.0000\n",
      "Epoch 416/20000, Loss: 28568546.0000\n",
      "Epoch 417/20000, Loss: 28295952.0000\n",
      "Epoch 418/20000, Loss: 28036432.0000\n",
      "Epoch 419/20000, Loss: 27786820.0000\n",
      "Epoch 420/20000, Loss: 27544732.0000\n",
      "Epoch 421/20000, Loss: 27308788.0000\n",
      "Epoch 422/20000, Loss: 27078676.0000\n",
      "Epoch 423/20000, Loss: 26855132.0000\n",
      "Epoch 424/20000, Loss: 26639980.0000\n",
      "Epoch 425/20000, Loss: 26436346.0000\n",
      "Epoch 426/20000, Loss: 26249076.0000\n",
      "Epoch 427/20000, Loss: 26085608.0000\n",
      "Epoch 428/20000, Loss: 25957380.0000\n",
      "Epoch 429/20000, Loss: 25882322.0000\n",
      "Epoch 430/20000, Loss: 25889004.0000\n",
      "Epoch 431/20000, Loss: 26023718.0000\n",
      "Epoch 432/20000, Loss: 26362642.0000\n",
      "Epoch 433/20000, Loss: 27032970.0000\n",
      "Epoch 434/20000, Loss: 28249956.0000\n",
      "Epoch 435/20000, Loss: 30382440.0000\n",
      "Epoch 436/20000, Loss: 34069120.0000\n",
      "Epoch 437/20000, Loss: 40424904.0000\n",
      "Epoch 438/20000, Loss: 51403144.0000\n",
      "Epoch 439/20000, Loss: 70413792.0000\n",
      "Epoch 440/20000, Loss: 103311744.0000\n",
      "Epoch 441/20000, Loss: 159748544.0000\n",
      "Epoch 442/20000, Loss: 254260528.0000\n",
      "Epoch 443/20000, Loss: 404522112.0000\n",
      "Epoch 444/20000, Loss: 620171328.0000\n",
      "Epoch 445/20000, Loss: 873436288.0000\n",
      "Epoch 446/20000, Loss: 1061259008.0000\n",
      "Epoch 447/20000, Loss: 1024911424.0000\n",
      "Epoch 448/20000, Loss: 699786368.0000\n",
      "Epoch 449/20000, Loss: 266445088.0000\n",
      "Epoch 450/20000, Loss: 28609588.0000\n",
      "Epoch 451/20000, Loss: 109658064.0000\n",
      "Epoch 452/20000, Loss: 355993344.0000\n",
      "Epoch 453/20000, Loss: 505800960.0000\n",
      "Epoch 454/20000, Loss: 416109568.0000\n",
      "Epoch 455/20000, Loss: 181675632.0000\n",
      "Epoch 456/20000, Loss: 27431858.0000\n",
      "Epoch 457/20000, Loss: 67430288.0000\n",
      "Epoch 458/20000, Loss: 209976112.0000\n",
      "Epoch 459/20000, Loss: 283920448.0000\n",
      "Epoch 460/20000, Loss: 215065904.0000\n",
      "Epoch 461/20000, Loss: 83224192.0000\n",
      "Epoch 462/20000, Loss: 18755640.0000\n",
      "Epoch 463/20000, Loss: 62249072.0000\n",
      "Epoch 464/20000, Loss: 141497792.0000\n",
      "Epoch 465/20000, Loss: 164282064.0000\n",
      "Epoch 466/20000, Loss: 111089120.0000\n",
      "Epoch 467/20000, Loss: 40750184.0000\n",
      "Epoch 468/20000, Loss: 18425810.0000\n",
      "Epoch 469/20000, Loss: 50899544.0000\n",
      "Epoch 470/20000, Loss: 92088352.0000\n",
      "Epoch 471/20000, Loss: 97273384.0000\n",
      "Epoch 472/20000, Loss: 64331336.0000\n",
      "Epoch 473/20000, Loss: 27249100.0000\n",
      "Epoch 474/20000, Loss: 17487652.0000\n",
      "Epoch 475/20000, Loss: 35737080.0000\n",
      "Epoch 476/20000, Loss: 57840580.0000\n",
      "Epoch 477/20000, Loss: 61361720.0000\n",
      "Epoch 478/20000, Loss: 44669544.0000\n",
      "Epoch 479/20000, Loss: 23987626.0000\n",
      "Epoch 480/20000, Loss: 15818887.0000\n",
      "Epoch 481/20000, Loss: 23012130.0000\n",
      "Epoch 482/20000, Loss: 35389712.0000\n",
      "Epoch 483/20000, Loss: 40823160.0000\n",
      "Epoch 484/20000, Loss: 35261256.0000\n",
      "Epoch 485/20000, Loss: 24127012.0000\n",
      "Epoch 486/20000, Loss: 16075172.0000\n",
      "Epoch 487/20000, Loss: 15720148.0000\n",
      "Epoch 488/20000, Loss: 21108356.0000\n",
      "Epoch 489/20000, Loss: 26617480.0000\n",
      "Epoch 490/20000, Loss: 27774240.0000\n",
      "Epoch 491/20000, Loss: 24098452.0000\n",
      "Epoch 492/20000, Loss: 18523936.0000\n",
      "Epoch 493/20000, Loss: 14658666.0000\n",
      "Epoch 494/20000, Loss: 14265598.0000\n",
      "Epoch 495/20000, Loss: 16561706.0000\n",
      "Epoch 496/20000, Loss: 19300528.0000\n",
      "Epoch 497/20000, Loss: 20477978.0000\n",
      "Epoch 498/20000, Loss: 19457928.0000\n",
      "Epoch 499/20000, Loss: 17013460.0000\n",
      "Epoch 500/20000, Loss: 14555625.0000\n",
      "Epoch 501/20000, Loss: 13206233.0000\n",
      "Epoch 502/20000, Loss: 13269386.0000\n",
      "Epoch 503/20000, Loss: 14273886.0000\n",
      "Epoch 504/20000, Loss: 15400349.0000\n",
      "Epoch 505/20000, Loss: 15964542.0000\n",
      "Epoch 506/20000, Loss: 15703744.0000\n",
      "Epoch 507/20000, Loss: 14788804.0000\n",
      "Epoch 508/20000, Loss: 13638066.0000\n",
      "Epoch 509/20000, Loss: 12675679.0000\n",
      "Epoch 510/20000, Loss: 12156104.0000\n",
      "Epoch 511/20000, Loss: 12108374.0000\n",
      "Epoch 512/20000, Loss: 12385260.0000\n",
      "Epoch 513/20000, Loss: 12764446.0000\n",
      "Epoch 514/20000, Loss: 13046350.0000\n",
      "Epoch 515/20000, Loss: 13114015.0000\n",
      "Epoch 516/20000, Loss: 12946687.0000\n",
      "Epoch 517/20000, Loss: 12598269.0000\n",
      "Epoch 518/20000, Loss: 12159680.0000\n",
      "Epoch 519/20000, Loss: 11722633.0000\n",
      "Epoch 520/20000, Loss: 11355189.0000\n",
      "Epoch 521/20000, Loss: 11092093.0000\n",
      "Epoch 522/20000, Loss: 10937125.0000\n",
      "Epoch 523/20000, Loss: 10872370.0000\n",
      "Epoch 524/20000, Loss: 10869344.0000\n",
      "Epoch 525/20000, Loss: 10898456.0000\n",
      "Epoch 526/20000, Loss: 10935182.0000\n",
      "Epoch 527/20000, Loss: 10962840.0000\n",
      "Epoch 528/20000, Loss: 10972779.0000\n",
      "Epoch 529/20000, Loss: 10962983.0000\n",
      "Epoch 530/20000, Loss: 10936140.0000\n",
      "Epoch 531/20000, Loss: 10897780.0000\n",
      "Epoch 532/20000, Loss: 10854894.0000\n",
      "Epoch 533/20000, Loss: 10815142.0000\n",
      "Epoch 534/20000, Loss: 10786668.0000\n",
      "Epoch 535/20000, Loss: 10778430.0000\n",
      "Epoch 536/20000, Loss: 10801024.0000\n",
      "Epoch 537/20000, Loss: 10868066.0000\n",
      "Epoch 538/20000, Loss: 10998278.0000\n",
      "Epoch 539/20000, Loss: 11218632.0000\n",
      "Epoch 540/20000, Loss: 11569175.0000\n",
      "Secret guessed correctly at epoch 540!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearComplex()"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearComplex(params)\n",
    "\n",
    "train_model(model, lwe_dataset, params, n_epochs=20000, lr=1, check_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = lwe_dataset.get_A()\n",
    "b = lwe_dataset.get_B()\n",
    "\n",
    "raw_guessed_secret = np.linalg.pinv(A) @ b\n",
    "raw_guessed_secret = torch.tensor(raw_guessed_secret, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Guessed secret: tensor([ 1.0398,  0.0269,  0.0503,  ...,  0.9741, -0.9928, -0.0079])\n",
      "Guessed secret: [ 1.  0.  0. ...  1. -1.  0.]\n",
      "Actual secret: [ 1  0  0 ...  1 -1  0]\n"
     ]
    }
   ],
   "source": [
    "# Check the guessed secret\n",
    "#raw_guessed_secret = model.guessed_secret.detach().cpu()\n",
    "guessed_secret = raw_guessed_secret.round().numpy()\n",
    "guessed_secret[guessed_secret == -0.0] = 0.0\n",
    "guessed_secret[guessed_secret > params['q'] // 2] -= params['q']\n",
    "\n",
    "real_secret = lwe_dataset.get_secret()\n",
    "real_secret[real_secret > params['q'] // 2] -= params['q']\n",
    "\n",
    "print(\"Raw Guessed secret:\", raw_guessed_secret)\n",
    "print(\"Guessed secret:\", guessed_secret)\n",
    "print(\"Actual secret:\", real_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the differences between the guessed and actual secret\n",
    "diff = guessed_secret - real_secret\n",
    "raw_diff = raw_guessed_secret[diff != 0]\n",
    "raw_diff[raw_diff > params['q'] // 2] -= params['q']\n",
    "if len(diff[diff != 0]) > 0:\n",
    "    print(\"Difference:\", raw_diff)\n",
    "    print(\"real_secret:\", real_secret[diff != 0])\n",
    "    print(\"guessed_secret:\", guessed_secret[diff != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of uncertain values: 0\n",
      "Number of brute force attempts required: 1\n",
      "Real uncertain secret: []\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Find values in raw_guessed_secret that are within Â±0.1 of an integer\n",
    "close_to_integer = torch.abs(raw_guessed_secret - torch.round(raw_guessed_secret)) < 0.4\n",
    "uncertain_count = torch.sum(~close_to_integer).item()\n",
    "print(\"Number of uncertain values:\", uncertain_count)\n",
    "\n",
    "# Calculate the number of brute force attacks to perform\n",
    "brute_force_attempts = 2 ** uncertain_count\n",
    "print(\"Number of brute force attempts required:\", brute_force_attempts)\n",
    "\n",
    "# Get the indices of uncertain values\n",
    "uncertain_indices = torch.where(~close_to_integer)[0]\n",
    "\n",
    "real_uncertain_secret = real_secret[uncertain_indices]\n",
    "print(\"Real uncertain secret:\", real_uncertain_secret)\n",
    "\n",
    "# Perform brute force attack\n",
    "raw_uncertain_secret = raw_guessed_secret[uncertain_indices]\n",
    "raw_uncertain_secret[raw_uncertain_secret > params['q'] // 2] -= params['q']\n",
    "raw_uncertain_secret = raw_uncertain_secret[torch.abs(raw_uncertain_secret) <= params['eta']]\n",
    "\n",
    "lower_values = torch.floor(raw_uncertain_secret).long()\n",
    "upper_values = torch.ceil(raw_uncertain_secret).long()\n",
    "\n",
    "#values = product(*zip(lower_values, upper_values))\n",
    "\n",
    "#for value in values:\n",
    "#    print(\"Trying values:\", value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "       |  -2.0  -1.0   0.0   1.0   2.0 | Accuracy\n",
      "-------------------------------------------------\n",
      "  -2.0 |    64     0     0     0     0 | 100.0%\n",
      "  -1.0 |     0   255     0     0     0 | 100.0%\n",
      "   0.0 |     0     0   382     0     0 | 100.0%\n",
      "   1.0 |     0     0     0   248     0 | 100.0%\n",
      "   2.0 |     0     0     0     0    75 | 100.0%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -2       1.00      1.00      1.00        64\n",
      "          -1       1.00      1.00      1.00       255\n",
      "           0       1.00      1.00      1.00       382\n",
      "           1       1.00      1.00      1.00       248\n",
      "           2       1.00      1.00      1.00        75\n",
      "\n",
      "    accuracy                           1.00      1024\n",
      "   macro avg       1.00      1.00      1.00      1024\n",
      "weighted avg       1.00      1.00      1.00      1024\n",
      "\n",
      "Mean Squared Error (MSE): 0.0000\n",
      "Mean Absolute Error (MAE): 0.0000\n"
     ]
    }
   ],
   "source": [
    "def report(real_secret, guessed_secret):\n",
    "    \"\"\"\n",
    "    Print classification report and confusion matrix.\n",
    "    \"\"\"\n",
    "  \n",
    "    # Get unique sorted labels and compute confusion matrix\n",
    "    labels = np.unique(np.concatenate((real_secret, guessed_secret)))\n",
    "    cm = confusion_matrix(real_secret, guessed_secret, labels=labels)\n",
    "\n",
    "    # Header\n",
    "    header = \"       |\" + \"\".join([f\"{l:>6}\" for l in labels]) + \" | Accuracy\"\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    # Rows\n",
    "    for i, row in enumerate(cm):\n",
    "        label = f\"{labels[i]:>6} |\"\n",
    "        values = \"\".join([f\"{v:6}\" for v in row])\n",
    "\n",
    "        correct = row[i]\n",
    "        total = row.sum()\n",
    "        acc = correct / total if total > 0 else 0.0\n",
    "        print(label + values + f\" | {acc:4.1%}\")\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(real_secret, guessed_secret, zero_division=0))\n",
    "\n",
    "    # Calculate Mean Squared Error (MSE)\n",
    "    mse = np.mean((real_secret - guessed_secret) ** 2)\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "\n",
    "    # Calculate Mean Absolute Error (MAE)\n",
    "    mae = np.mean(np.abs(real_secret - guessed_secret))\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "report(real_secret, guessed_secret)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlwe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
