{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# own code\n",
    "from kyber_py import *\n",
    "from fourier import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLWEDataset(Dataset):\n",
    "    def __init__(self, params, num_samples):\n",
    "        \"\"\"\n",
    "        params for the MLWE scheme\n",
    "        \"\"\"\n",
    "        self.mlwe = MLWE(params)\n",
    "        self.samples = []\n",
    "        random_byte = self.mlwe.get_random_bytes()\n",
    "        secret = self.mlwe.generate_secret(random_byte)\n",
    "        secret_hat = secret.to_ntt()\n",
    "        for i in range(num_samples):\n",
    "            updated_byte = self._increase_byte(random_byte, i)\n",
    "            #A_hat, B_hat = self.mlwe.generate_A_B_hat(secret_hat, updated_byte)\n",
    "            A_hat = self.mlwe.generate_A_hat(updated_byte)\n",
    "            B_hat = A_hat @ secret_hat\n",
    "            \n",
    "            A_tensor = torch.tensor(A_hat.to_list()).to(dtype=torch.float64)\n",
    "            B_tensor = torch.tensor(B_hat.to_list()).to(dtype=torch.float64)\n",
    "            self.samples.append((A_tensor, B_tensor))\n",
    "\n",
    "        self.secret = torch.tensor(secret.to_list()).float()\n",
    "\n",
    "    def _increase_byte(self, input_bytes, N):\n",
    "        return (int.from_bytes(input_bytes, byteorder='big') + N).to_bytes(len(input_bytes), byteorder='big')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.samples[index]\n",
    "    \n",
    "    def get_secret(self):\n",
    "        return self.secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with NTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n': 8,\n",
    "    'q': 17,\n",
    "    'k': 1,\n",
    "    'secret_type': 'binary',\n",
    "    'error_type': 'binary',\n",
    "    'seed': 0,\n",
    "    'hw': 3\n",
    "}\n",
    "\n",
    "mlwe_dataset = MLWEDataset(params, 1024)\n",
    "mlwe_loader = DataLoader(mlwe_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural solver model for Module-LWE using both Fourier mapping and FFT transformation.\n",
    "class MLWESolver(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        \"\"\"\n",
    "        n: Secret dimension (e.g., 8)\n",
    "        q: Modulus\n",
    "        \"\"\"\n",
    "        super(MLWESolver, self).__init__()\n",
    "        self.q = params['q']\n",
    "        self.n = params['n']\n",
    "        self.k = params['k']\n",
    "        self.secret_type = params['secret_type']\n",
    "        self.error_type = params['error_type']\n",
    "        \n",
    "        root = find_root_of_unity(self.n, self.q)\n",
    "        if root is None:\n",
    "            raise ValueError(\"Root of unity not found for the given n and q.\")\n",
    "        \n",
    "        self.ntt_zetas = [pow(root, br(i, int(math.log2(self.n))-1), self.q) for i in range(self.n // 2)]\n",
    "        self.ntt_zetas = torch.tensor(self.ntt_zetas, dtype=torch.float64)\n",
    "        \n",
    "        self.ntt_f = pow(self.n // 2, -1, self.q)\n",
    "\n",
    "        self.guessed_secret = nn.Parameter(nn.init.xavier_normal_(torch.empty(self.k, 1, self.n), gain=1.0))\n",
    "\n",
    "    def forward(self, A_batch):\n",
    "        \"\"\"\n",
    "        A: Public matrix, shape (batch, 2, 8) with integer entries.\n",
    "        B: Ground truth vector, shape (batch, 1, 8) with integer entries.\n",
    "        Returns:\n",
    "          pred_B: Predicted Fourier-FFT representation of B, shape (batch, 1, 8, 2)\n",
    "          B_target: Ground truth Fourier-FFT representation of B, shape (batch, 1, 8, 2)\n",
    "          s_hat: Current estimate of the secret.\n",
    "        \"\"\"\n",
    "        # --- Process s_hat ---\n",
    "        # Map the trainable secret to complex via Fourier mapping.\n",
    "        s_complex = fourier_int_to_complex(self.guessed_secret, self.q)  # shape: (8,), complex\n",
    "        s_complex_ntt = fourier_ntt(s_complex, self.ntt_zetas)\n",
    "\n",
    "        # --- Multiply in the FFT domain ---\n",
    "        # Reshape A_batch and s_complex_ntt to handle batch size\n",
    "        result = torch.stack([fourier_matmul(A_hat, s_complex_ntt, self.ntt_zetas) for A_hat in A_batch])\n",
    "\n",
    "        result = fourier_complex_to_int(result, self.q)\n",
    "    \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Name: guessed_secret\n",
      "Requires Grad: True\n",
      "Shape: torch.Size([1, 1, 8])\n",
      "Values: tensor([[[ 0.5964,  0.4165, -0.1928, -0.2692,  0.4453, -0.6868, -0.5268,\n",
      "           0.1806]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = MLWESolver(params)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "  print(f\"Parameter Name: {name}\")\n",
    "  print(f\"Requires Grad: {param.requires_grad}\")\n",
    "  print(f\"Shape: {param.shape}\")\n",
    "  print(f\"Values: {param.data}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 0., 1., 0., 0., 1., 0.]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlwe_dataset.get_secret()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.672230064868927\n"
     ]
    }
   ],
   "source": [
    "mse = nn.MSELoss()\n",
    "loss_secret = mse(model.guessed_secret, mlwe_dataset.get_secret())\n",
    "print(f\"Loss: {loss_secret.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, n_epochs=100, lr=1e-3):\n",
    "    # Get secret dimension from first sample.\n",
    "    secret = dataloader.dataset.get_secret()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for A_hat, B_hat in dataloader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred_B = model(A_hat)\n",
    "\n",
    "            loss = criterion(pred_B, B_hat)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        \n",
    "        # Calculate loss between the secret and s_hat\n",
    "        s_loss = criterion(model.guessed_secret, secret)\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{n_epochs} - Loss: {avg_loss:.6f} - Secret Loss: {s_loss.item():.6f}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Loss: 109.264784 - Secret Loss: 0.749209\n",
      "Epoch 2/100 - Loss: 111.600398 - Secret Loss: 0.733288\n",
      "Epoch 3/100 - Loss: 112.104864 - Secret Loss: 0.717980\n",
      "Epoch 4/100 - Loss: 111.640498 - Secret Loss: 0.704605\n",
      "Epoch 5/100 - Loss: 110.221347 - Secret Loss: 0.694326\n",
      "Epoch 6/100 - Loss: 113.190556 - Secret Loss: 0.683880\n",
      "Epoch 7/100 - Loss: 110.973790 - Secret Loss: 0.676738\n",
      "Epoch 8/100 - Loss: 111.989337 - Secret Loss: 0.665154\n",
      "Epoch 9/100 - Loss: 112.597793 - Secret Loss: 0.656788\n",
      "Epoch 10/100 - Loss: 113.094740 - Secret Loss: 0.649535\n",
      "Epoch 11/100 - Loss: 111.309226 - Secret Loss: 0.642847\n",
      "Epoch 12/100 - Loss: 112.508550 - Secret Loss: 0.638679\n",
      "Epoch 13/100 - Loss: 112.506064 - Secret Loss: 0.631703\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model = MLWESolver(params)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m trained_model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlwe_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, dataloader, n_epochs, lr)\u001b[39m\n\u001b[32m     13\u001b[39m pred_B = model(A_hat)\n\u001b[32m     15\u001b[39m loss = criterion(pred_B, B_hat)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m optimizer.step()\n\u001b[32m     19\u001b[39m epoch_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlwe/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlwe/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlwe/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = MLWESolver(params)\n",
    "trained_model = train_model(model, mlwe_loader, n_epochs=100, lr=1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlwe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
