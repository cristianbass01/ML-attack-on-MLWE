{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kyber_py import *\n",
    "from fourier import *\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import sympy as sp\n",
    "from sympy import ntheory\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import circulant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n': 8,\n",
    "    'q': 17,\n",
    "    'k': 2,\n",
    "    'secret_type': 'binary',\n",
    "    'error_type': 'binary',\n",
    "    'seed': 0,\n",
    "    'hw': 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_circ(a):\n",
    "    \"\"\"\n",
    "    Generates a negative circulant matrix from the input vector a.\n",
    "    \"\"\"\n",
    "    n = len(a)\n",
    "    A = circulant(a)\n",
    "    tri = np.triu_indices(n, 1)\n",
    "    A[tri] *= -1\n",
    "    return A\n",
    "\n",
    "n = params['n']\n",
    "q = params['q']\n",
    "k = params['k']\n",
    "l = params['l'] if 'l' in params else k\n",
    "\n",
    "# Generate the MLWE matrices\n",
    "mlwe = MLWE(params)\n",
    "random_bytes = mlwe.get_random_bytes()\n",
    "result = mlwe.generate(random_bytes)\n",
    "A, s, e, B = result\n",
    "# The generated LWE parameters will have shape (kn, ln)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 16])\n",
      "tensor([[  1,  -5, -10, -11,  -2, -16, -12, -10],\n",
      "        [ 10,   1,  -5, -10, -11,  -2, -16, -12],\n",
      "        [ 12,  10,   1,  -5, -10, -11,  -2, -16],\n",
      "        [ 16,  12,  10,   1,  -5, -10, -11,  -2],\n",
      "        [  2,  16,  12,  10,   1,  -5, -10, -11],\n",
      "        [ 11,   2,  16,  12,  10,   1,  -5, -10],\n",
      "        [ 10,  11,   2,  16,  12,  10,   1,  -5],\n",
      "        [  5,  10,  11,   2,  16,  12,  10,   1]], dtype=torch.int32)\n",
      "tensor([[ 12,  -4,  -1, -10,  -4, -16,  -2, -11],\n",
      "        [ 11,  12,  -4,  -1, -10,  -4, -16,  -2],\n",
      "        [  2,  11,  12,  -4,  -1, -10,  -4, -16],\n",
      "        [ 16,   2,  11,  12,  -4,  -1, -10,  -4],\n",
      "        [  4,  16,   2,  11,  12,  -4,  -1, -10],\n",
      "        [ 10,   4,  16,   2,  11,  12,  -4,  -1],\n",
      "        [  1,  10,   4,  16,   2,  11,  12,  -4],\n",
      "        [  4,   1,  10,   4,  16,   2,  11,  12]], dtype=torch.int32)\n",
      "tensor([[ 12, -12, -11, -10, -16, -16, -10, -13],\n",
      "        [ 13,  12, -12, -11, -10, -16, -16, -10],\n",
      "        [ 10,  13,  12, -12, -11, -10, -16, -16],\n",
      "        [ 16,  10,  13,  12, -12, -11, -10, -16],\n",
      "        [ 16,  16,  10,  13,  12, -12, -11, -10],\n",
      "        [ 10,  16,  16,  10,  13,  12, -12, -11],\n",
      "        [ 11,  10,  16,  16,  10,  13,  12, -12],\n",
      "        [ 12,  11,  10,  16,  16,  10,  13,  12]], dtype=torch.int32)\n",
      "tensor([[  8,  -1,  -1, -11,  -3,   0,  -3,  -5],\n",
      "        [  5,   8,  -1,  -1, -11,  -3,   0,  -3],\n",
      "        [  3,   5,   8,  -1,  -1, -11,  -3,   0],\n",
      "        [  0,   3,   5,   8,  -1,  -1, -11,  -3],\n",
      "        [  3,   0,   3,   5,   8,  -1,  -1, -11],\n",
      "        [ 11,   3,   0,   3,   5,   8,  -1,  -1],\n",
      "        [  1,  11,   3,   0,   3,   5,   8,  -1],\n",
      "        [  1,   1,  11,   3,   0,   3,   5,   8]], dtype=torch.int32)\n",
      "A:  [[[1, 10, 12, 16, 2, 11, 10, 5], [12, 13, 10, 16, 16, 10, 11, 12]], [[12, 11, 2, 16, 4, 10, 1, 4], [8, 5, 3, 0, 3, 11, 1, 1]]]\n"
     ]
    }
   ],
   "source": [
    "A_lwe = torch.zeros((k*n, l*n), dtype=torch.int32)\n",
    "for i in range(k): # rows\n",
    "  for j in range(l): # columns\n",
    "    a = A[i,j].to_list()\n",
    "    neg_circ_a = neg_circ(a)\n",
    "    A_lwe[i*n:(i+1)*n, j*n:(j+1)*n] = torch.from_numpy(neg_circ_a)\n",
    "\n",
    "print(A_lwe.shape)\n",
    "print(A_lwe[0:8, 0:8])\n",
    "print(A_lwe[8:16, 0:8])\n",
    "print(A_lwe[0:8, 8:16])\n",
    "print(A_lwe[8:16, 8:16])\n",
    "print(\"A: \", A.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_lwe shape: torch.Size([16, 1])\n",
      "s_lwe: tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1]], dtype=torch.int32)\n",
      "s:  [[[0, 0, 0, 1, 1, 0, 0, 1]], [[1, 1, 0, 0, 0, 0, 0, 1]]]\n"
     ]
    }
   ],
   "source": [
    "# Transform the secret vector\n",
    "s_lwe = torch.tensor(s.to_list(), dtype=torch.int32)\n",
    "s_lwe = s_lwe.reshape(n*l, 1)\n",
    "print(\"s_lwe shape:\", s_lwe.shape)\n",
    "print(\"s_lwe:\", s_lwe)\n",
    "print(\"s: \", s.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_lwe shape: torch.Size([16, 1])\n",
      "b_lwe: tensor([[15],\n",
      "        [16],\n",
      "        [10],\n",
      "        [ 4],\n",
      "        [ 6],\n",
      "        [10],\n",
      "        [16],\n",
      "        [ 3],\n",
      "        [11],\n",
      "        [14],\n",
      "        [ 5],\n",
      "        [ 5],\n",
      "        [ 5],\n",
      "        [ 9],\n",
      "        [ 8],\n",
      "        [ 8]], dtype=torch.int32)\n",
      "B:  [[[15, 16, 10, 4, 6, 10, 16, 3]], [[11, 14, 5, 5, 5, 9, 8, 8]]]\n"
     ]
    }
   ],
   "source": [
    "b_lwe = torch.tensor(B.to_list(), dtype=torch.int32)\n",
    "b_lwe = b_lwe.reshape(n*k, 1)\n",
    "print(\"b_lwe shape:\", b_lwe.shape)\n",
    "print(\"b_lwe:\", b_lwe)\n",
    "print(\"B: \", B.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_lwe shape: torch.Size([16, 1])\n",
      "e_lwe: tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0]], dtype=torch.int32)\n",
      "e:  [[[0, 0, 0, 0, 1, 0, 1, 0]], [[0, 0, 1, 1, 0, 1, 0, 0]]]\n"
     ]
    }
   ],
   "source": [
    "e_lwe = torch.tensor(e.to_list(), dtype=torch.int32)\n",
    "e_lwe = e_lwe.reshape(n*k, 1)\n",
    "print(\"e_lwe shape:\", e_lwe.shape)\n",
    "print(\"e_lwe:\", e_lwe)\n",
    "print(\"e: \", e.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check correctness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(A_lwe @ s_lwe + e_lwe) % q == b_lwe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to transform MLWE and RLWE into LWE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_circ(a):\n",
    "    \"\"\"\n",
    "    Generates a negative circulant matrix from the input vector a.\n",
    "    \"\"\"\n",
    "    n = len(a)\n",
    "    A = circulant(a)\n",
    "    tri = np.triu_indices(n, 1)\n",
    "    A[tri] *= -1\n",
    "    return A\n",
    "\n",
    "def get_lwe(params):\n",
    "  n = params['n']\n",
    "  k = params['k']\n",
    "  l = params['l'] if 'l' in params else k\n",
    "\n",
    "  # Generate the MLWE matrices\n",
    "  mlwe = MLWE(params)\n",
    "  random_bytes = mlwe.get_random_bytes()\n",
    "  A, s, e, B = mlwe.generate(random_bytes)\n",
    "  \n",
    "  # Transform A:\n",
    "  A_lwe = torch.zeros((k*n, l*n), dtype=torch.int32)\n",
    "  for i in range(k): # rows\n",
    "    for j in range(l): # columns\n",
    "      a = A[i,j].to_list()\n",
    "      neg_circ_a = neg_circ(a)\n",
    "      A_lwe[i*n:(i+1)*n, j*n:(j+1)*n] = torch.from_numpy(neg_circ_a)\n",
    "\n",
    "  # Transform s:\n",
    "  s_lwe = torch.tensor(s.to_list(), dtype=torch.int32)\n",
    "  s_lwe = s_lwe.reshape(n*l, 1)\n",
    "\n",
    "  # Transform B:\n",
    "  b_lwe = torch.tensor(B.to_list(), dtype=torch.int32)\n",
    "  b_lwe = b_lwe.reshape(n*k, 1)\n",
    "\n",
    "  # Transform e:\n",
    "  e_lwe = torch.tensor(e.to_list(), dtype=torch.int32)\n",
    "  e_lwe = e_lwe.reshape(n*k, 1)\n",
    "\n",
    "  return A_lwe, s_lwe, b_lwe, e_lwe\n",
    "\n",
    "def transform_lwe(A : list, b : list):\n",
    "  k = len(A)\n",
    "  l = len(A[0])\n",
    "  n = len(A[0][0])\n",
    "\n",
    "  # Transform A:\n",
    "  A_lwe = torch.zeros((k*n, l*n), dtype=torch.int32)\n",
    "  for i in range(k): # rows\n",
    "    for j in range(l): # columns\n",
    "      a = A[i][j]\n",
    "      neg_circ_a = neg_circ(a)\n",
    "      A_lwe[i*n:(i+1)*n, j*n:(j+1)*n] = torch.from_numpy(neg_circ_a)\n",
    "\n",
    "  # Transform b:\n",
    "  b_lwe = torch.tensor(b, dtype=torch.int32)\n",
    "  b_lwe = b_lwe.reshape(n*k, 1)\n",
    "  \n",
    "  return A_lwe, b_lwe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret:  [0, 1, 0, 1, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'n': 8,\n",
    "    'q': 17,\n",
    "    'k': 1,\n",
    "    'secret_type': 'binary',\n",
    "    'error_type': 'binary',\n",
    "    'seed': 0,\n",
    "    'hw': 3,\n",
    "}\n",
    "A_lwe, s_lwe, b_lwe, e_lwe = get_lwe(params)\n",
    "print(\"secret: \", s_lwe.flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LWEDataset(Dataset):\n",
    "    def __init__(self, params, num_samples=1000):\n",
    "        \"\"\"\n",
    "        params for the LWE scheme\n",
    "        \"\"\"\n",
    "        n = params['n']\n",
    "        k = params['k']\n",
    "\n",
    "        self.mlwe = MLWE(params)\n",
    "        num_gen = num_samples // (n*k) + 1\n",
    "\n",
    "        self.A = torch.zeros((num_gen *n*k, k*n), dtype=torch.float64)\n",
    "        self.B = torch.zeros((num_gen *n*k), dtype=torch.float64)\n",
    "        \n",
    "        random_byte = self.mlwe.get_random_bytes()\n",
    "        secret = self.mlwe.generate_secret(random_byte)\n",
    "\n",
    "        for i in range(num_gen):\n",
    "            updated_byte = self._increase_byte(random_byte, i)\n",
    "            A, B = self.mlwe.generate_A_B(secret, updated_byte)\n",
    "            A_lwe, B_lwe = transform_lwe(A.to_list(), B.to_list())\n",
    "            self.A[i*n*k:(i+1)*n*k, :] = A_lwe.squeeze()\n",
    "            self.B[i*n*k:(i+1)*n*k] = B_lwe.squeeze()\n",
    "\n",
    "        self.A = self.A[:num_samples, :]\n",
    "        self.B = self.B[:num_samples]\n",
    "        self.secret = torch.tensor(secret.to_list(), dtype=torch.float64).squeeze()\n",
    "\n",
    "    def _increase_byte(self, input_bytes, N):\n",
    "        return (int.from_bytes(input_bytes, byteorder='big') + N).to_bytes(len(input_bytes), byteorder='big')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.A)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.A[index], self.B[index]\n",
    "    \n",
    "    def get_secret(self):\n",
    "        return self.secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n': 50,\n",
    "    'q': 3329,\n",
    "    'k': 1,\n",
    "    'secret_type': 'binary',\n",
    "    'error_type': 'binary',\n",
    "    'seed': 0,\n",
    "    'hw': 1\n",
    "}\n",
    "\n",
    "lwe_dataset = LWEDataset(params, num_samples=100000)\n",
    "lwe_dataloader = DataLoader(lwe_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training debug:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural solver model for Module-LWE using both Fourier mapping and FFT transformation.\n",
    "class SimpleSolver(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        \"\"\"\n",
    "        n: Secret dimension (e.g., 8)\n",
    "        q: Modulus\n",
    "        \"\"\"\n",
    "        super(SimpleSolver, self).__init__()\n",
    "        self.q = params['q']\n",
    "        self.n = params['n']\n",
    "        self.k = params['k']\n",
    "        self.secret_type = params['secret_type']\n",
    "        self.error_type = params['error_type']\n",
    "        \n",
    "        self.guessed_secret = nn.Parameter(nn.init.normal_(torch.empty(self.n, dtype=torch.float64), mean=0.5, std=0.5), requires_grad=True)\n",
    "\n",
    "    def forward(self, A_batch, B_batch=None):\n",
    "        \n",
    "        # --- Process s_hat ---\n",
    "        s_complex = fourier_int_to_complex(self.guessed_secret, self.q)  # shape: (8,), complex\n",
    "        \n",
    "        result_complex = s_complex ** A_batch\n",
    "        \n",
    "        result_complex = torch.prod(result_complex, dim=1)\n",
    "\n",
    "        # Transform also B_batch to complex if present (for easier loss evaluation)\n",
    "        if B_batch is not None:\n",
    "            return result_complex, fourier_int_to_complex(B_batch, self.q)\n",
    "        else:\n",
    "            return result_complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parabolic_regularization(weights):\n",
    "    return torch.mean(torch.pow(torch.pow(weights, 2) - weights, 2))\n",
    "\n",
    "def angular_loss(pred, target):\n",
    "    \"\"\"\n",
    "    Calculates the angular distance between two complex tensors.\n",
    "    The angular distance is defined as 1 - cos(theta), where theta is the angle between the two complex numbers.\n",
    "    Maximum loss = 1\n",
    "    \"\"\"\n",
    "    pred_angle = torch.angle(pred)\n",
    "    target_angle = torch.angle(target)\n",
    "    angular_distance = 1 - torch.cos(pred_angle - target_angle)\n",
    "    return torch.mean(angular_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 0.9994718788430906\n",
      "Regularization loss: 0.14554728711308396\n",
      "Initial total loss: 1.1450191659561746\n",
      "Initial secret loss: 0.5611939316359915\n"
     ]
    }
   ],
   "source": [
    "# Calculate the initial loss for the model\n",
    "model = SimpleSolver(params)\n",
    "pred_B, target_B = model(lwe_dataset.A, lwe_dataset.B)\n",
    "loss = angular_loss(pred_B, target_B)\n",
    "print(\"Initial loss:\", loss.item())\n",
    "\n",
    "# Calculate the initial regularization loss\n",
    "regularization_loss = parabolic_regularization(model.guessed_secret)\n",
    "print(\"Regularization loss:\", regularization_loss.item())\n",
    "\n",
    "# Calculate the total loss\n",
    "total_loss = loss + regularization_loss\n",
    "print(\"Initial total loss:\", total_loss.item())\n",
    "\n",
    "# Calculate the secret loss\n",
    "secret_loss = torch.mean(torch.pow(model.guessed_secret - lwe_dataset.secret, 2))\n",
    "print(\"Initial secret loss:\", secret_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global minimum loss: 8.592832149590702e-07\n"
     ]
    }
   ],
   "source": [
    "# Calculate the global minimum of the loss function\n",
    "A_lwe, b_lwe = lwe_dataset.A, lwe_dataset.B\n",
    "real_secret = lwe_dataset.get_secret()\n",
    "\n",
    "s_complex = fourier_int_to_complex(real_secret, params['q'])\n",
    "        \n",
    "result_complex = s_complex ** A_lwe\n",
    "        \n",
    "result_complex = torch.prod(result_complex, dim=1)\n",
    "\n",
    "# Calculate the loss\n",
    "loss = angular_loss(result_complex, fourier_int_to_complex(b_lwe, params['q']))\n",
    "print(\"Global minimum loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_loss(main_loss, regularization, step, max_step):\n",
    "    # Sigmoid-based schedule\n",
    "    reg_weight = 1 / (1 + math.exp(-((step / max_step) * 10 - 5)))\n",
    "    \n",
    "    return main_loss + reg_weight * regularization * 50\n",
    "\n",
    "def train_model(model, dataloader, log_interval=100, n_epochs=10, lr=0.01):\n",
    "    # Get secret dimension from first sample.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    # Get the secret from the dataset\n",
    "    secret = dataloader.dataset.get_secret()\n",
    "    secret = secret.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        step_loss = 0.0\n",
    "        for step, (A_batch, B_batch) in enumerate(dataloader):\n",
    "            A_batch = A_batch.to(device)\n",
    "            B_batch = B_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred_B_complex, B_complex = model(A_batch, B_batch)\n",
    "\n",
    "            b_loss = angular_loss(pred_B_complex, B_complex)\n",
    "\n",
    "            # Regularization term\n",
    "            reg_loss = parabolic_regularization(model.guessed_secret)\n",
    "            \n",
    "            # Combine losses\n",
    "            total_loss = compute_total_loss(b_loss, reg_loss, step, len(dataloader))\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            step_loss += total_loss.item()\n",
    "\n",
    "            if step % log_interval == 0:\n",
    "                avg_loss = step_loss / log_interval\n",
    "                step_loss = 0.0\n",
    "\n",
    "                # Calculate loss between the secret and s_hat\n",
    "                s_loss = criterion(model.guessed_secret, secret)\n",
    "                print(f\"Epoch [{epoch}/{n_epochs}], Step [{step}/{len(dataloader)}], \"\n",
    "                      f\"Loss: {avg_loss:.4f}, Secret Loss: {s_loss.item():.4f}, \"\n",
    "                      f\"Regularization Loss: {reg_loss.item():.4f}\")\n",
    "\n",
    "        \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [0/12500], Loss: 0.0109, Secret Loss: 0.5656, Regularization Loss: 0.1455\n",
      "Epoch [1/10], Step [100/12500], Loss: 1.0517, Secret Loss: 0.5873, Regularization Loss: 0.1306\n",
      "Epoch [1/10], Step [200/12500], Loss: 1.0490, Secret Loss: 0.5946, Regularization Loss: 0.1190\n",
      "Epoch [1/10], Step [300/12500], Loss: 1.0722, Secret Loss: 0.5135, Regularization Loss: 0.0967\n",
      "Epoch [1/10], Step [400/12500], Loss: 1.0618, Secret Loss: 0.4024, Regularization Loss: 0.1265\n",
      "Epoch [1/10], Step [500/12500], Loss: 1.0277, Secret Loss: 0.4229, Regularization Loss: 0.0851\n",
      "Epoch [1/10], Step [600/12500], Loss: 1.0482, Secret Loss: 0.4145, Regularization Loss: 0.0778\n",
      "Epoch [1/10], Step [700/12500], Loss: 1.0859, Secret Loss: 0.3978, Regularization Loss: 0.0795\n",
      "Epoch [1/10], Step [800/12500], Loss: 1.0607, Secret Loss: 0.4068, Regularization Loss: 0.0638\n",
      "Epoch [1/10], Step [900/12500], Loss: 1.0857, Secret Loss: 0.3921, Regularization Loss: 0.0752\n",
      "Epoch [1/10], Step [1000/12500], Loss: 1.0515, Secret Loss: 0.3629, Regularization Loss: 0.0704\n",
      "Epoch [1/10], Step [1100/12500], Loss: 1.0798, Secret Loss: 0.4064, Regularization Loss: 0.0464\n",
      "Epoch [1/10], Step [1200/12500], Loss: 1.0297, Secret Loss: 0.4311, Regularization Loss: 0.0453\n",
      "Epoch [1/10], Step [1300/12500], Loss: 1.0427, Secret Loss: 0.3690, Regularization Loss: 0.0326\n",
      "Epoch [1/10], Step [1400/12500], Loss: 0.9861, Secret Loss: 0.3384, Regularization Loss: 0.0272\n",
      "Epoch [1/10], Step [1500/12500], Loss: 1.0439, Secret Loss: 0.3364, Regularization Loss: 0.0281\n",
      "Epoch [1/10], Step [1600/12500], Loss: 1.0823, Secret Loss: 0.2629, Regularization Loss: 0.0462\n",
      "Epoch [1/10], Step [1700/12500], Loss: 1.0395, Secret Loss: 0.2857, Regularization Loss: 0.0367\n",
      "Epoch [1/10], Step [1800/12500], Loss: 1.0670, Secret Loss: 0.2689, Regularization Loss: 0.0465\n",
      "Epoch [1/10], Step [1900/12500], Loss: 1.0722, Secret Loss: 0.2851, Regularization Loss: 0.0398\n",
      "Epoch [1/10], Step [2000/12500], Loss: 1.0349, Secret Loss: 0.2743, Regularization Loss: 0.0315\n",
      "Epoch [1/10], Step [2100/12500], Loss: 1.0400, Secret Loss: 0.3040, Regularization Loss: 0.0304\n",
      "Epoch [1/10], Step [2200/12500], Loss: 1.0176, Secret Loss: 0.3111, Regularization Loss: 0.0232\n",
      "Epoch [1/10], Step [2300/12500], Loss: 1.0558, Secret Loss: 0.3238, Regularization Loss: 0.0417\n",
      "Epoch [1/10], Step [2400/12500], Loss: 1.1197, Secret Loss: 0.3424, Regularization Loss: 0.0535\n",
      "Epoch [1/10], Step [2500/12500], Loss: 1.0694, Secret Loss: 0.3636, Regularization Loss: 0.0534\n",
      "Epoch [1/10], Step [2600/12500], Loss: 1.1451, Secret Loss: 0.4241, Regularization Loss: 0.0548\n",
      "Epoch [1/10], Step [2700/12500], Loss: 1.1063, Secret Loss: 0.3321, Regularization Loss: 0.0260\n",
      "Epoch [1/10], Step [2800/12500], Loss: 1.0788, Secret Loss: 0.3209, Regularization Loss: 0.0318\n",
      "Epoch [1/10], Step [2900/12500], Loss: 1.0669, Secret Loss: 0.2940, Regularization Loss: 0.0275\n",
      "Epoch [1/10], Step [3000/12500], Loss: 1.1000, Secret Loss: 0.2277, Regularization Loss: 0.0407\n",
      "Epoch [1/10], Step [3100/12500], Loss: 1.1283, Secret Loss: 0.2316, Regularization Loss: 0.0339\n",
      "Epoch [1/10], Step [3200/12500], Loss: 1.1331, Secret Loss: 0.2103, Regularization Loss: 0.0293\n",
      "Epoch [1/10], Step [3300/12500], Loss: 1.1505, Secret Loss: 0.2057, Regularization Loss: 0.0317\n",
      "Epoch [1/10], Step [3400/12500], Loss: 1.1133, Secret Loss: 0.2404, Regularization Loss: 0.0209\n",
      "Epoch [1/10], Step [3500/12500], Loss: 1.1610, Secret Loss: 0.2387, Regularization Loss: 0.0221\n",
      "Epoch [1/10], Step [3600/12500], Loss: 1.1535, Secret Loss: 0.2161, Regularization Loss: 0.0261\n",
      "Epoch [1/10], Step [3700/12500], Loss: 1.1211, Secret Loss: 0.2779, Regularization Loss: 0.0213\n",
      "Epoch [1/10], Step [3800/12500], Loss: 1.1535, Secret Loss: 0.2802, Regularization Loss: 0.0164\n",
      "Epoch [1/10], Step [3900/12500], Loss: 1.1003, Secret Loss: 0.2876, Regularization Loss: 0.0150\n",
      "Epoch [1/10], Step [4000/12500], Loss: 1.1494, Secret Loss: 0.2903, Regularization Loss: 0.0146\n",
      "Epoch [1/10], Step [4100/12500], Loss: 1.0750, Secret Loss: 0.2746, Regularization Loss: 0.0121\n",
      "Epoch [1/10], Step [4200/12500], Loss: 1.0933, Secret Loss: 0.2527, Regularization Loss: 0.0105\n",
      "Epoch [1/10], Step [4300/12500], Loss: 1.0682, Secret Loss: 0.2750, Regularization Loss: 0.0080\n",
      "Epoch [1/10], Step [4400/12500], Loss: 1.0540, Secret Loss: 0.2917, Regularization Loss: 0.0071\n",
      "Epoch [1/10], Step [4500/12500], Loss: 1.0921, Secret Loss: 0.3058, Regularization Loss: 0.0073\n",
      "Epoch [1/10], Step [4600/12500], Loss: 1.1351, Secret Loss: 0.2717, Regularization Loss: 0.0127\n",
      "Epoch [1/10], Step [4700/12500], Loss: 1.1335, Secret Loss: 0.2907, Regularization Loss: 0.0116\n",
      "Epoch [1/10], Step [4800/12500], Loss: 1.1743, Secret Loss: 0.3062, Regularization Loss: 0.0103\n",
      "Epoch [1/10], Step [4900/12500], Loss: 1.1472, Secret Loss: 0.3302, Regularization Loss: 0.0086\n",
      "Epoch [1/10], Step [5000/12500], Loss: 1.0955, Secret Loss: 0.3685, Regularization Loss: 0.0111\n",
      "Epoch [1/10], Step [5100/12500], Loss: 1.1163, Secret Loss: 0.3313, Regularization Loss: 0.0055\n",
      "Epoch [1/10], Step [5200/12500], Loss: 1.1322, Secret Loss: 0.3893, Regularization Loss: 0.0138\n",
      "Epoch [1/10], Step [5300/12500], Loss: 1.2516, Secret Loss: 0.4183, Regularization Loss: 0.0203\n",
      "Epoch [1/10], Step [5400/12500], Loss: 1.2646, Secret Loss: 0.3916, Regularization Loss: 0.0116\n",
      "Epoch [1/10], Step [5500/12500], Loss: 1.1201, Secret Loss: 0.3113, Regularization Loss: 0.0102\n",
      "Epoch [1/10], Step [5600/12500], Loss: 1.1894, Secret Loss: 0.3467, Regularization Loss: 0.0062\n",
      "Epoch [1/10], Step [5700/12500], Loss: 1.0703, Secret Loss: 0.3618, Regularization Loss: 0.0064\n",
      "Epoch [1/10], Step [5800/12500], Loss: 1.0707, Secret Loss: 0.3152, Regularization Loss: 0.0060\n",
      "Epoch [1/10], Step [5900/12500], Loss: 1.0855, Secret Loss: 0.3486, Regularization Loss: 0.0039\n",
      "Epoch [1/10], Step [6000/12500], Loss: 1.1339, Secret Loss: 0.3592, Regularization Loss: 0.0035\n",
      "Epoch [1/10], Step [6100/12500], Loss: 1.0409, Secret Loss: 0.3606, Regularization Loss: 0.0030\n",
      "Epoch [1/10], Step [6200/12500], Loss: 1.0916, Secret Loss: 0.3702, Regularization Loss: 0.0020\n",
      "Epoch [1/10], Step [6300/12500], Loss: 1.0943, Secret Loss: 0.3506, Regularization Loss: 0.0026\n",
      "Epoch [1/10], Step [6400/12500], Loss: 1.0777, Secret Loss: 0.3495, Regularization Loss: 0.0019\n",
      "Epoch [1/10], Step [6500/12500], Loss: 1.0722, Secret Loss: 0.3764, Regularization Loss: 0.0032\n",
      "Epoch [1/10], Step [6600/12500], Loss: 1.0978, Secret Loss: 0.3404, Regularization Loss: 0.0022\n",
      "Epoch [1/10], Step [6700/12500], Loss: 1.0958, Secret Loss: 0.3515, Regularization Loss: 0.0016\n",
      "Epoch [1/10], Step [6800/12500], Loss: 1.1373, Secret Loss: 0.3357, Regularization Loss: 0.0027\n",
      "Epoch [1/10], Step [6900/12500], Loss: 1.0569, Secret Loss: 0.3350, Regularization Loss: 0.0014\n",
      "Epoch [1/10], Step [7000/12500], Loss: 1.0916, Secret Loss: 0.3949, Regularization Loss: 0.0029\n",
      "Epoch [1/10], Step [7100/12500], Loss: 1.2004, Secret Loss: 0.3700, Regularization Loss: 0.0026\n",
      "Epoch [1/10], Step [7200/12500], Loss: 1.0308, Secret Loss: 0.3510, Regularization Loss: 0.0020\n",
      "Epoch [1/10], Step [7300/12500], Loss: 1.1036, Secret Loss: 0.3552, Regularization Loss: 0.0029\n",
      "Epoch [1/10], Step [7400/12500], Loss: 1.0715, Secret Loss: 0.3462, Regularization Loss: 0.0021\n",
      "Epoch [1/10], Step [7500/12500], Loss: 1.0738, Secret Loss: 0.3769, Regularization Loss: 0.0021\n",
      "Epoch [1/10], Step [7600/12500], Loss: 1.0279, Secret Loss: 0.3576, Regularization Loss: 0.0028\n",
      "Epoch [1/10], Step [7700/12500], Loss: 1.1874, Secret Loss: 0.3708, Regularization Loss: 0.0039\n",
      "Epoch [1/10], Step [7800/12500], Loss: 1.1002, Secret Loss: 0.3502, Regularization Loss: 0.0015\n",
      "Epoch [1/10], Step [7900/12500], Loss: 1.0947, Secret Loss: 0.3597, Regularization Loss: 0.0043\n",
      "Epoch [1/10], Step [8000/12500], Loss: 1.1031, Secret Loss: 0.3647, Regularization Loss: 0.0015\n",
      "Epoch [1/10], Step [8100/12500], Loss: 1.0896, Secret Loss: 0.3644, Regularization Loss: 0.0015\n",
      "Epoch [1/10], Step [8200/12500], Loss: 1.1853, Secret Loss: 0.3329, Regularization Loss: 0.0028\n",
      "Epoch [1/10], Step [8300/12500], Loss: 1.0987, Secret Loss: 0.3904, Regularization Loss: 0.0038\n",
      "Epoch [1/10], Step [8400/12500], Loss: 1.0678, Secret Loss: 0.3654, Regularization Loss: 0.0074\n",
      "Epoch [1/10], Step [8500/12500], Loss: 1.2764, Secret Loss: 0.3786, Regularization Loss: 0.0032\n",
      "Epoch [1/10], Step [8600/12500], Loss: 1.1252, Secret Loss: 0.3547, Regularization Loss: 0.0026\n",
      "Epoch [1/10], Step [8700/12500], Loss: 1.0712, Secret Loss: 0.3584, Regularization Loss: 0.0006\n",
      "Epoch [1/10], Step [8800/12500], Loss: 1.0646, Secret Loss: 0.3618, Regularization Loss: 0.0011\n",
      "Epoch [1/10], Step [8900/12500], Loss: 1.0683, Secret Loss: 0.3665, Regularization Loss: 0.0021\n",
      "Epoch [1/10], Step [9000/12500], Loss: 1.1076, Secret Loss: 0.3523, Regularization Loss: 0.0014\n",
      "Epoch [1/10], Step [9100/12500], Loss: 1.0439, Secret Loss: 0.3596, Regularization Loss: 0.0029\n",
      "Epoch [1/10], Step [9200/12500], Loss: 1.0951, Secret Loss: 0.3876, Regularization Loss: 0.0035\n",
      "Epoch [1/10], Step [9300/12500], Loss: 1.1063, Secret Loss: 0.3672, Regularization Loss: 0.0015\n",
      "Epoch [1/10], Step [9400/12500], Loss: 1.0893, Secret Loss: 0.3495, Regularization Loss: 0.0014\n",
      "Epoch [1/10], Step [9500/12500], Loss: 1.0834, Secret Loss: 0.3680, Regularization Loss: 0.0013\n",
      "Epoch [1/10], Step [9600/12500], Loss: 1.0808, Secret Loss: 0.3578, Regularization Loss: 0.0010\n",
      "Epoch [1/10], Step [9700/12500], Loss: 1.0751, Secret Loss: 0.3474, Regularization Loss: 0.0015\n",
      "Epoch [1/10], Step [9800/12500], Loss: 1.0803, Secret Loss: 0.3819, Regularization Loss: 0.0035\n",
      "Epoch [1/10], Step [9900/12500], Loss: 1.2066, Secret Loss: 0.3881, Regularization Loss: 0.0037\n",
      "Epoch [1/10], Step [10000/12500], Loss: 1.0704, Secret Loss: 0.3725, Regularization Loss: 0.0020\n",
      "Epoch [1/10], Step [10100/12500], Loss: 1.0065, Secret Loss: 0.3598, Regularization Loss: 0.0016\n",
      "Epoch [1/10], Step [10200/12500], Loss: 1.0339, Secret Loss: 0.3798, Regularization Loss: 0.0021\n",
      "Epoch [1/10], Step [10300/12500], Loss: 1.1861, Secret Loss: 0.3319, Regularization Loss: 0.0030\n",
      "Epoch [1/10], Step [10400/12500], Loss: 1.1198, Secret Loss: 0.3586, Regularization Loss: 0.0037\n",
      "Epoch [1/10], Step [10500/12500], Loss: 1.0668, Secret Loss: 0.3563, Regularization Loss: 0.0033\n",
      "Epoch [1/10], Step [10600/12500], Loss: 1.0554, Secret Loss: 0.3839, Regularization Loss: 0.0033\n",
      "Epoch [1/10], Step [10700/12500], Loss: 1.1114, Secret Loss: 0.3523, Regularization Loss: 0.0018\n",
      "Epoch [1/10], Step [10800/12500], Loss: 1.1112, Secret Loss: 0.3519, Regularization Loss: 0.0007\n",
      "Epoch [1/10], Step [10900/12500], Loss: 1.0815, Secret Loss: 0.3716, Regularization Loss: 0.0022\n",
      "Epoch [1/10], Step [11000/12500], Loss: 1.1006, Secret Loss: 0.3384, Regularization Loss: 0.0023\n",
      "Epoch [1/10], Step [11100/12500], Loss: 1.0464, Secret Loss: 0.3688, Regularization Loss: 0.0016\n",
      "Epoch [1/10], Step [11200/12500], Loss: 1.0764, Secret Loss: 0.3508, Regularization Loss: 0.0010\n",
      "Epoch [1/10], Step [11300/12500], Loss: 1.1657, Secret Loss: 0.3595, Regularization Loss: 0.0009\n",
      "Epoch [1/10], Step [11400/12500], Loss: 1.1946, Secret Loss: 0.3658, Regularization Loss: 0.0050\n",
      "Epoch [1/10], Step [11500/12500], Loss: 1.1172, Secret Loss: 0.3355, Regularization Loss: 0.0021\n",
      "Epoch [1/10], Step [11600/12500], Loss: 1.0783, Secret Loss: 0.3674, Regularization Loss: 0.0013\n",
      "Epoch [1/10], Step [11700/12500], Loss: 1.0817, Secret Loss: 0.3621, Regularization Loss: 0.0009\n",
      "Epoch [1/10], Step [11800/12500], Loss: 1.1392, Secret Loss: 0.3359, Regularization Loss: 0.0024\n",
      "Epoch [1/10], Step [11900/12500], Loss: 1.0945, Secret Loss: 0.3656, Regularization Loss: 0.0012\n",
      "Epoch [1/10], Step [12000/12500], Loss: 1.0743, Secret Loss: 0.3702, Regularization Loss: 0.0009\n",
      "Epoch [1/10], Step [12100/12500], Loss: 1.0652, Secret Loss: 0.3525, Regularization Loss: 0.0012\n",
      "Epoch [1/10], Step [12200/12500], Loss: 1.0823, Secret Loss: 0.3483, Regularization Loss: 0.0014\n",
      "Epoch [1/10], Step [12300/12500], Loss: 1.1334, Secret Loss: 0.3667, Regularization Loss: 0.0018\n",
      "Epoch [1/10], Step [12400/12500], Loss: 1.1118, Secret Loss: 0.3487, Regularization Loss: 0.0023\n",
      "Epoch [2/10], Step [0/12500], Loss: 0.0111, Secret Loss: 0.3601, Regularization Loss: 0.0010\n",
      "Epoch [2/10], Step [100/12500], Loss: 1.0178, Secret Loss: 0.3519, Regularization Loss: 0.0062\n",
      "Epoch [2/10], Step [200/12500], Loss: 1.0175, Secret Loss: 0.3271, Regularization Loss: 0.0183\n",
      "Epoch [2/10], Step [300/12500], Loss: 0.9971, Secret Loss: 0.2957, Regularization Loss: 0.0564\n",
      "Epoch [2/10], Step [400/12500], Loss: 1.0204, Secret Loss: 0.2873, Regularization Loss: 0.0694\n",
      "Epoch [2/10], Step [500/12500], Loss: 1.0379, Secret Loss: 0.2974, Regularization Loss: 0.0526\n",
      "Epoch [2/10], Step [600/12500], Loss: 1.0591, Secret Loss: 0.2992, Regularization Loss: 0.0642\n",
      "Epoch [2/10], Step [700/12500], Loss: 1.0071, Secret Loss: 0.3372, Regularization Loss: 0.0408\n",
      "Epoch [2/10], Step [800/12500], Loss: 1.0484, Secret Loss: 0.3350, Regularization Loss: 0.0798\n",
      "Epoch [2/10], Step [900/12500], Loss: 1.0277, Secret Loss: 0.3296, Regularization Loss: 0.0663\n",
      "Epoch [2/10], Step [1000/12500], Loss: 1.0650, Secret Loss: 0.3084, Regularization Loss: 0.0693\n",
      "Epoch [2/10], Step [1100/12500], Loss: 1.0389, Secret Loss: 0.3320, Regularization Loss: 0.0633\n",
      "Epoch [2/10], Step [1200/12500], Loss: 1.0542, Secret Loss: 0.3230, Regularization Loss: 0.0825\n",
      "Epoch [2/10], Step [1300/12500], Loss: 1.0317, Secret Loss: 0.3253, Regularization Loss: 0.0400\n",
      "Epoch [2/10], Step [1400/12500], Loss: 1.0067, Secret Loss: 0.3435, Regularization Loss: 0.0436\n",
      "Epoch [2/10], Step [1500/12500], Loss: 1.0361, Secret Loss: 0.3767, Regularization Loss: 0.0417\n",
      "Epoch [2/10], Step [1600/12500], Loss: 1.0238, Secret Loss: 0.3836, Regularization Loss: 0.0432\n",
      "Epoch [2/10], Step [1700/12500], Loss: 1.0232, Secret Loss: 0.4322, Regularization Loss: 0.0503\n",
      "Epoch [2/10], Step [1800/12500], Loss: 1.0311, Secret Loss: 0.5485, Regularization Loss: 0.0618\n",
      "Epoch [2/10], Step [1900/12500], Loss: 1.0330, Secret Loss: 0.4754, Regularization Loss: 0.0395\n",
      "Epoch [2/10], Step [2000/12500], Loss: 1.0683, Secret Loss: 0.4799, Regularization Loss: 0.0343\n",
      "Epoch [2/10], Step [2100/12500], Loss: 1.0652, Secret Loss: 0.4480, Regularization Loss: 0.0322\n",
      "Epoch [2/10], Step [2200/12500], Loss: 1.0513, Secret Loss: 0.4222, Regularization Loss: 0.0265\n",
      "Epoch [2/10], Step [2300/12500], Loss: 1.0246, Secret Loss: 0.4411, Regularization Loss: 0.0315\n",
      "Epoch [2/10], Step [2400/12500], Loss: 1.0867, Secret Loss: 0.4414, Regularization Loss: 0.0219\n",
      "Epoch [2/10], Step [2500/12500], Loss: 1.0476, Secret Loss: 0.4318, Regularization Loss: 0.0268\n",
      "Epoch [2/10], Step [2600/12500], Loss: 1.0297, Secret Loss: 0.4520, Regularization Loss: 0.0246\n",
      "Epoch [2/10], Step [2700/12500], Loss: 1.0986, Secret Loss: 0.5010, Regularization Loss: 0.0311\n",
      "Epoch [2/10], Step [2800/12500], Loss: 1.1263, Secret Loss: 0.4969, Regularization Loss: 0.0225\n",
      "Epoch [2/10], Step [2900/12500], Loss: 1.0534, Secret Loss: 0.4694, Regularization Loss: 0.0189\n",
      "Epoch [2/10], Step [3000/12500], Loss: 1.0278, Secret Loss: 0.5188, Regularization Loss: 0.0175\n",
      "Epoch [2/10], Step [3100/12500], Loss: 1.0565, Secret Loss: 0.6276, Regularization Loss: 0.0306\n",
      "Epoch [2/10], Step [3200/12500], Loss: 1.1534, Secret Loss: 0.5916, Regularization Loss: 0.0241\n",
      "Epoch [2/10], Step [3300/12500], Loss: 1.1073, Secret Loss: 0.5781, Regularization Loss: 0.0393\n",
      "Epoch [2/10], Step [3400/12500], Loss: 1.2134, Secret Loss: 0.5012, Regularization Loss: 0.0352\n",
      "Epoch [2/10], Step [3500/12500], Loss: 1.1558, Secret Loss: 0.5476, Regularization Loss: 0.0434\n",
      "Epoch [2/10], Step [3600/12500], Loss: 1.2644, Secret Loss: 0.6075, Regularization Loss: 0.0479\n",
      "Epoch [2/10], Step [3700/12500], Loss: 1.2241, Secret Loss: 0.5906, Regularization Loss: 0.0273\n",
      "Epoch [2/10], Step [3800/12500], Loss: 1.1655, Secret Loss: 0.5641, Regularization Loss: 0.0196\n",
      "Epoch [2/10], Step [3900/12500], Loss: 1.1407, Secret Loss: 0.6336, Regularization Loss: 0.0248\n",
      "Epoch [2/10], Step [4000/12500], Loss: 1.1786, Secret Loss: 0.6230, Regularization Loss: 0.0232\n",
      "Epoch [2/10], Step [4100/12500], Loss: 1.1717, Secret Loss: 0.7011, Regularization Loss: 0.0307\n",
      "Epoch [2/10], Step [4200/12500], Loss: 1.1548, Secret Loss: 0.7225, Regularization Loss: 0.0241\n",
      "Epoch [2/10], Step [4300/12500], Loss: 1.1140, Secret Loss: 0.6306, Regularization Loss: 0.0146\n",
      "Epoch [2/10], Step [4400/12500], Loss: 1.1473, Secret Loss: 0.6379, Regularization Loss: 0.0152\n",
      "Epoch [2/10], Step [4500/12500], Loss: 1.1621, Secret Loss: 0.5745, Regularization Loss: 0.0123\n",
      "Epoch [2/10], Step [4600/12500], Loss: 1.1495, Secret Loss: 0.5948, Regularization Loss: 0.0108\n",
      "Epoch [2/10], Step [4700/12500], Loss: 1.1033, Secret Loss: 0.6559, Regularization Loss: 0.0143\n",
      "Epoch [2/10], Step [4800/12500], Loss: 1.1091, Secret Loss: 0.6026, Regularization Loss: 0.0082\n",
      "Epoch [2/10], Step [4900/12500], Loss: 1.1252, Secret Loss: 0.6563, Regularization Loss: 0.0065\n",
      "Epoch [2/10], Step [5000/12500], Loss: 1.0393, Secret Loss: 0.6542, Regularization Loss: 0.0055\n",
      "Epoch [2/10], Step [5100/12500], Loss: 1.0871, Secret Loss: 0.7251, Regularization Loss: 0.0094\n",
      "Epoch [2/10], Step [5200/12500], Loss: 1.1563, Secret Loss: 0.6361, Regularization Loss: 0.0060\n",
      "Epoch [2/10], Step [5300/12500], Loss: 1.0803, Secret Loss: 0.5944, Regularization Loss: 0.0067\n",
      "Epoch [2/10], Step [5400/12500], Loss: 1.0979, Secret Loss: 0.6192, Regularization Loss: 0.0059\n",
      "Epoch [2/10], Step [5500/12500], Loss: 1.1341, Secret Loss: 0.6325, Regularization Loss: 0.0064\n",
      "Epoch [2/10], Step [5600/12500], Loss: 1.1148, Secret Loss: 0.6804, Regularization Loss: 0.0054\n",
      "Epoch [2/10], Step [5700/12500], Loss: 1.1731, Secret Loss: 0.7893, Regularization Loss: 0.0152\n",
      "Epoch [2/10], Step [5800/12500], Loss: 1.2272, Secret Loss: 0.7180, Regularization Loss: 0.0054\n",
      "Epoch [2/10], Step [5900/12500], Loss: 1.0684, Secret Loss: 0.6778, Regularization Loss: 0.0028\n",
      "Epoch [2/10], Step [6000/12500], Loss: 1.0245, Secret Loss: 0.6165, Regularization Loss: 0.0029\n",
      "Epoch [2/10], Step [6100/12500], Loss: 1.0635, Secret Loss: 0.6420, Regularization Loss: 0.0029\n",
      "Epoch [2/10], Step [6200/12500], Loss: 1.1105, Secret Loss: 0.6461, Regularization Loss: 0.0027\n",
      "Epoch [2/10], Step [6300/12500], Loss: 1.0325, Secret Loss: 0.6438, Regularization Loss: 0.0025\n",
      "Epoch [2/10], Step [6400/12500], Loss: 1.0835, Secret Loss: 0.6527, Regularization Loss: 0.0038\n",
      "Epoch [2/10], Step [6500/12500], Loss: 1.0366, Secret Loss: 0.6781, Regularization Loss: 0.0017\n",
      "Epoch [2/10], Step [6600/12500], Loss: 1.0832, Secret Loss: 0.6558, Regularization Loss: 0.0028\n",
      "Epoch [2/10], Step [6700/12500], Loss: 1.1410, Secret Loss: 0.6566, Regularization Loss: 0.0030\n",
      "Epoch [2/10], Step [6800/12500], Loss: 1.0896, Secret Loss: 0.6152, Regularization Loss: 0.0061\n",
      "Epoch [2/10], Step [6900/12500], Loss: 1.2147, Secret Loss: 0.6911, Regularization Loss: 0.0038\n",
      "Epoch [2/10], Step [7000/12500], Loss: 1.1505, Secret Loss: 0.6275, Regularization Loss: 0.0035\n",
      "Epoch [2/10], Step [7100/12500], Loss: 1.1488, Secret Loss: 0.6868, Regularization Loss: 0.0019\n",
      "Epoch [2/10], Step [7200/12500], Loss: 1.0769, Secret Loss: 0.6667, Regularization Loss: 0.0018\n",
      "Epoch [2/10], Step [7300/12500], Loss: 1.0753, Secret Loss: 0.6056, Regularization Loss: 0.0039\n",
      "Epoch [2/10], Step [7400/12500], Loss: 1.1406, Secret Loss: 0.6128, Regularization Loss: 0.0040\n",
      "Epoch [2/10], Step [7500/12500], Loss: 1.0807, Secret Loss: 0.6788, Regularization Loss: 0.0013\n",
      "Epoch [2/10], Step [7600/12500], Loss: 1.0999, Secret Loss: 0.6758, Regularization Loss: 0.0055\n",
      "Epoch [2/10], Step [7700/12500], Loss: 1.0852, Secret Loss: 0.6565, Regularization Loss: 0.0009\n",
      "Epoch [2/10], Step [7800/12500], Loss: 1.0845, Secret Loss: 0.6520, Regularization Loss: 0.0022\n",
      "Epoch [2/10], Step [7900/12500], Loss: 1.0563, Secret Loss: 0.6803, Regularization Loss: 0.0013\n",
      "Epoch [2/10], Step [8000/12500], Loss: 1.0668, Secret Loss: 0.6993, Regularization Loss: 0.0030\n",
      "Epoch [2/10], Step [8100/12500], Loss: 1.0809, Secret Loss: 0.6957, Regularization Loss: 0.0021\n",
      "Epoch [2/10], Step [8200/12500], Loss: 1.0859, Secret Loss: 0.6502, Regularization Loss: 0.0010\n",
      "Epoch [2/10], Step [8300/12500], Loss: 1.0864, Secret Loss: 0.6750, Regularization Loss: 0.0023\n",
      "Epoch [2/10], Step [8400/12500], Loss: 1.0350, Secret Loss: 0.6813, Regularization Loss: 0.0012\n",
      "Epoch [2/10], Step [8500/12500], Loss: 1.0866, Secret Loss: 0.7077, Regularization Loss: 0.0025\n",
      "Epoch [2/10], Step [8600/12500], Loss: 1.1169, Secret Loss: 0.6882, Regularization Loss: 0.0016\n",
      "Epoch [2/10], Step [8700/12500], Loss: 1.1499, Secret Loss: 0.7107, Regularization Loss: 0.0049\n",
      "Epoch [2/10], Step [8800/12500], Loss: 1.1169, Secret Loss: 0.6308, Regularization Loss: 0.0025\n",
      "Epoch [2/10], Step [8900/12500], Loss: 1.0953, Secret Loss: 0.6208, Regularization Loss: 0.0025\n",
      "Epoch [2/10], Step [9000/12500], Loss: 1.0679, Secret Loss: 0.7076, Regularization Loss: 0.0033\n",
      "Epoch [2/10], Step [9100/12500], Loss: 1.1412, Secret Loss: 0.6155, Regularization Loss: 0.0017\n",
      "Epoch [2/10], Step [9200/12500], Loss: 1.0992, Secret Loss: 0.6122, Regularization Loss: 0.0039\n",
      "Epoch [2/10], Step [9300/12500], Loss: 1.1289, Secret Loss: 0.6870, Regularization Loss: 0.0019\n",
      "Epoch [2/10], Step [9400/12500], Loss: 1.1469, Secret Loss: 0.6946, Regularization Loss: 0.0025\n",
      "Epoch [2/10], Step [9500/12500], Loss: 1.1743, Secret Loss: 0.6562, Regularization Loss: 0.0040\n",
      "Epoch [2/10], Step [9600/12500], Loss: 1.2039, Secret Loss: 0.6735, Regularization Loss: 0.0037\n",
      "Epoch [2/10], Step [9700/12500], Loss: 1.0181, Secret Loss: 0.7143, Regularization Loss: 0.0036\n",
      "Epoch [2/10], Step [9800/12500], Loss: 1.1492, Secret Loss: 0.6499, Regularization Loss: 0.0012\n",
      "Epoch [2/10], Step [9900/12500], Loss: 1.1445, Secret Loss: 0.6777, Regularization Loss: 0.0024\n",
      "Epoch [2/10], Step [10000/12500], Loss: 1.0344, Secret Loss: 0.6576, Regularization Loss: 0.0015\n",
      "Epoch [2/10], Step [10100/12500], Loss: 1.1107, Secret Loss: 0.5860, Regularization Loss: 0.0083\n",
      "Epoch [2/10], Step [10200/12500], Loss: 1.1855, Secret Loss: 0.6342, Regularization Loss: 0.0018\n",
      "Epoch [2/10], Step [10300/12500], Loss: 1.1124, Secret Loss: 0.6914, Regularization Loss: 0.0025\n",
      "Epoch [2/10], Step [10400/12500], Loss: 1.0435, Secret Loss: 0.7170, Regularization Loss: 0.0034\n",
      "Epoch [2/10], Step [10500/12500], Loss: 1.1080, Secret Loss: 0.6341, Regularization Loss: 0.0023\n",
      "Epoch [2/10], Step [10600/12500], Loss: 1.1999, Secret Loss: 0.6135, Regularization Loss: 0.0026\n",
      "Epoch [2/10], Step [10700/12500], Loss: 1.0646, Secret Loss: 0.6580, Regularization Loss: 0.0007\n",
      "Epoch [2/10], Step [10800/12500], Loss: 1.0387, Secret Loss: 0.6346, Regularization Loss: 0.0021\n",
      "Epoch [2/10], Step [10900/12500], Loss: 1.0993, Secret Loss: 0.7074, Regularization Loss: 0.0032\n",
      "Epoch [2/10], Step [11000/12500], Loss: 1.1847, Secret Loss: 0.6238, Regularization Loss: 0.0016\n",
      "Epoch [2/10], Step [11100/12500], Loss: 1.0489, Secret Loss: 0.6116, Regularization Loss: 0.0022\n",
      "Epoch [2/10], Step [11200/12500], Loss: 1.0657, Secret Loss: 0.7037, Regularization Loss: 0.0018\n",
      "Epoch [2/10], Step [11300/12500], Loss: 1.0862, Secret Loss: 0.6324, Regularization Loss: 0.0047\n",
      "Epoch [2/10], Step [11400/12500], Loss: 1.1649, Secret Loss: 0.6277, Regularization Loss: 0.0015\n",
      "Epoch [2/10], Step [11500/12500], Loss: 1.0352, Secret Loss: 0.6502, Regularization Loss: 0.0008\n",
      "Epoch [2/10], Step [11600/12500], Loss: 1.0705, Secret Loss: 0.6629, Regularization Loss: 0.0007\n",
      "Epoch [2/10], Step [11700/12500], Loss: 1.0489, Secret Loss: 0.6272, Regularization Loss: 0.0022\n",
      "Epoch [2/10], Step [11800/12500], Loss: 1.1052, Secret Loss: 0.6623, Regularization Loss: 0.0010\n",
      "Epoch [2/10], Step [11900/12500], Loss: 1.2086, Secret Loss: 0.7034, Regularization Loss: 0.0047\n",
      "Epoch [2/10], Step [12000/12500], Loss: 1.1339, Secret Loss: 0.6175, Regularization Loss: 0.0020\n",
      "Epoch [2/10], Step [12100/12500], Loss: 1.1167, Secret Loss: 0.6310, Regularization Loss: 0.0026\n",
      "Epoch [2/10], Step [12200/12500], Loss: 1.1320, Secret Loss: 0.6890, Regularization Loss: 0.0024\n",
      "Epoch [2/10], Step [12300/12500], Loss: 1.1356, Secret Loss: 0.6480, Regularization Loss: 0.0012\n",
      "Epoch [2/10], Step [12400/12500], Loss: 1.1465, Secret Loss: 0.6172, Regularization Loss: 0.0031\n",
      "Epoch [3/10], Step [0/12500], Loss: 0.0118, Secret Loss: 0.6072, Regularization Loss: 0.0025\n",
      "Epoch [3/10], Step [100/12500], Loss: 1.0167, Secret Loss: 0.6532, Regularization Loss: 0.0032\n",
      "Epoch [3/10], Step [200/12500], Loss: 1.0305, Secret Loss: 0.7271, Regularization Loss: 0.0106\n",
      "Epoch [3/10], Step [300/12500], Loss: 1.0038, Secret Loss: 0.6815, Regularization Loss: 0.0134\n",
      "Epoch [3/10], Step [400/12500], Loss: 1.0319, Secret Loss: 0.7531, Regularization Loss: 0.0237\n",
      "Epoch [3/10], Step [500/12500], Loss: 1.0145, Secret Loss: 0.6805, Regularization Loss: 0.0269\n",
      "Epoch [3/10], Step [600/12500], Loss: 0.9809, Secret Loss: 0.6767, Regularization Loss: 0.0302\n",
      "Epoch [3/10], Step [700/12500], Loss: 1.0347, Secret Loss: 0.6464, Regularization Loss: 0.0333\n",
      "Epoch [3/10], Step [800/12500], Loss: 1.0415, Secret Loss: 0.7209, Regularization Loss: 0.0420\n",
      "Epoch [3/10], Step [900/12500], Loss: 0.9874, Secret Loss: 0.6916, Regularization Loss: 0.0517\n",
      "Epoch [3/10], Step [1000/12500], Loss: 1.0815, Secret Loss: 0.8115, Regularization Loss: 0.0896\n",
      "Epoch [3/10], Step [1100/12500], Loss: 1.1012, Secret Loss: 0.7440, Regularization Loss: 0.0678\n",
      "Epoch [3/10], Step [1200/12500], Loss: 1.0360, Secret Loss: 0.7053, Regularization Loss: 0.0603\n",
      "Epoch [3/10], Step [1300/12500], Loss: 1.0893, Secret Loss: 0.7414, Regularization Loss: 0.0694\n",
      "Epoch [3/10], Step [1400/12500], Loss: 1.0196, Secret Loss: 0.7702, Regularization Loss: 0.0712\n",
      "Epoch [3/10], Step [1500/12500], Loss: 1.1161, Secret Loss: 0.8070, Regularization Loss: 0.0752\n",
      "Epoch [3/10], Step [1600/12500], Loss: 1.0754, Secret Loss: 0.7146, Regularization Loss: 0.0403\n",
      "Epoch [3/10], Step [1700/12500], Loss: 1.0575, Secret Loss: 0.6689, Regularization Loss: 0.0385\n",
      "Epoch [3/10], Step [1800/12500], Loss: 1.0662, Secret Loss: 0.7365, Regularization Loss: 0.0639\n",
      "Epoch [3/10], Step [1900/12500], Loss: 1.1052, Secret Loss: 0.6968, Regularization Loss: 0.0527\n",
      "Epoch [3/10], Step [2000/12500], Loss: 1.0776, Secret Loss: 0.6840, Regularization Loss: 0.0664\n",
      "Epoch [3/10], Step [2100/12500], Loss: 1.1052, Secret Loss: 0.7338, Regularization Loss: 0.0655\n",
      "Epoch [3/10], Step [2200/12500], Loss: 1.1118, Secret Loss: 0.7323, Regularization Loss: 0.0561\n",
      "Epoch [3/10], Step [2300/12500], Loss: 1.0588, Secret Loss: 0.6196, Regularization Loss: 0.0210\n",
      "Epoch [3/10], Step [2400/12500], Loss: 1.0334, Secret Loss: 0.5680, Regularization Loss: 0.0173\n",
      "Epoch [3/10], Step [2500/12500], Loss: 1.0270, Secret Loss: 0.5497, Regularization Loss: 0.0288\n",
      "Epoch [3/10], Step [2600/12500], Loss: 1.0884, Secret Loss: 0.6296, Regularization Loss: 0.0326\n",
      "Epoch [3/10], Step [2700/12500], Loss: 1.1094, Secret Loss: 0.6657, Regularization Loss: 0.0352\n",
      "Epoch [3/10], Step [2800/12500], Loss: 1.1293, Secret Loss: 0.7047, Regularization Loss: 0.0414\n",
      "Epoch [3/10], Step [2900/12500], Loss: 1.1482, Secret Loss: 0.6902, Regularization Loss: 0.0384\n",
      "Epoch [3/10], Step [3000/12500], Loss: 1.1736, Secret Loss: 0.6557, Regularization Loss: 0.0324\n",
      "Epoch [3/10], Step [3100/12500], Loss: 1.0992, Secret Loss: 0.6019, Regularization Loss: 0.0340\n",
      "Epoch [3/10], Step [3200/12500], Loss: 1.1466, Secret Loss: 0.6431, Regularization Loss: 0.0534\n",
      "Epoch [3/10], Step [3300/12500], Loss: 1.2193, Secret Loss: 0.6133, Regularization Loss: 0.0356\n",
      "Epoch [3/10], Step [3400/12500], Loss: 1.1721, Secret Loss: 0.5738, Regularization Loss: 0.0320\n",
      "Epoch [3/10], Step [3500/12500], Loss: 1.0877, Secret Loss: 0.4868, Regularization Loss: 0.0215\n",
      "Epoch [3/10], Step [3600/12500], Loss: 1.1352, Secret Loss: 0.4973, Regularization Loss: 0.0214\n",
      "Epoch [3/10], Step [3700/12500], Loss: 1.1638, Secret Loss: 0.4481, Regularization Loss: 0.0206\n",
      "Epoch [3/10], Step [3800/12500], Loss: 1.1615, Secret Loss: 0.4690, Regularization Loss: 0.0198\n",
      "Epoch [3/10], Step [3900/12500], Loss: 1.1620, Secret Loss: 0.5673, Regularization Loss: 0.0188\n",
      "Epoch [3/10], Step [4000/12500], Loss: 1.1101, Secret Loss: 0.4583, Regularization Loss: 0.0188\n",
      "Epoch [3/10], Step [4100/12500], Loss: 1.1108, Secret Loss: 0.4398, Regularization Loss: 0.0192\n",
      "Epoch [3/10], Step [4200/12500], Loss: 1.1219, Secret Loss: 0.4631, Regularization Loss: 0.0195\n",
      "Epoch [3/10], Step [4300/12500], Loss: 1.1503, Secret Loss: 0.5153, Regularization Loss: 0.0151\n",
      "Epoch [3/10], Step [4400/12500], Loss: 1.1205, Secret Loss: 0.5253, Regularization Loss: 0.0148\n",
      "Epoch [3/10], Step [4500/12500], Loss: 1.1125, Secret Loss: 0.5027, Regularization Loss: 0.0128\n",
      "Epoch [3/10], Step [4600/12500], Loss: 1.1988, Secret Loss: 0.5160, Regularization Loss: 0.0208\n",
      "Epoch [3/10], Step [4700/12500], Loss: 1.2551, Secret Loss: 0.5420, Regularization Loss: 0.0179\n",
      "Epoch [3/10], Step [4800/12500], Loss: 1.1529, Secret Loss: 0.5857, Regularization Loss: 0.0143\n",
      "Epoch [3/10], Step [4900/12500], Loss: 1.1988, Secret Loss: 0.5876, Regularization Loss: 0.0116\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlwe_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, dataloader, log_interval, n_epochs, lr)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Combine losses\u001b[39;00m\n\u001b[32m     34\u001b[39m total_loss = compute_total_loss(b_loss, reg_loss, step, \u001b[38;5;28mlen\u001b[39m(dataloader))\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mtotal_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m optimizer.step()\n\u001b[32m     38\u001b[39m step_loss += total_loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlwe/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlwe/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlwe/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_model(model, lwe_dataloader, log_interval=500, n_epochs=10, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guessed secret: [0.28077018 0.6506888  0.11807197 1.0863562  0.14392364 0.4651821\n",
      " 0.267927   0.08936887]\n",
      "Actual secret: [0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Check the guessed secret\n",
    "guessed_secret = model.guessed_secret.detach().cpu().numpy()\n",
    "print(\"Guessed secret:\", guessed_secret)\n",
    "print(\"Actual secret:\", lwe_dataset.get_secret().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlwe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
